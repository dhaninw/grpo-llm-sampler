{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1: Setup: Install Libraries ---\n",
        "print(\"--- Installing Libraries ---\")\n",
        "\n",
        "# Uninstall potentially conflicting versions first\n",
        "#!pip uninstall torch torchvision torchaudio transformers accelerate bitsandbytes -y -q\n",
        "print(\"--- Uninstalled existing versions ---\")\n",
        "\n",
        "# Reinstall PyTorch, torchvision, and torchaudio together to ensure compatibility\n",
        "# Let pip determine the compatible versions based on your Colab environment's CUDA\n",
        "print(\"--- Installing PyTorch, torchvision, torchaudio ---\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "# (Using cu121 as Colab often uses CUDA 12.1. Adjust if your environment differs, though Colab usually manages this well)\n",
        "\n",
        "# Install specific or minimum versions for other critical libraries\n",
        "print(\"--- Installing transformers, accelerate, bitsandbytes ---\")\n",
        "!pip install transformers>=4.51.0 -q\n",
        "!pip install accelerate>=0.28.0 -q\n",
        "!pip install bitsandbytes>=0.41.3 -q\n",
        "!pip install sentencepiece -q\n",
        "\n",
        "print(\"\\n--- Checking Installed Library Versions ---\")\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "print(f\"BitsandBytes version: {bitsandbytes.__version__}\")\n",
        "print(\"--- Libraries Installed ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef9FL4aKqbj4",
        "outputId": "397a5e38-9bde-489e-bd19-1fb42f3cac6c"
      },
      "id": "ef9FL4aKqbj4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing Libraries ---\n",
            "--- Uninstalled existing versions ---\n",
            "--- Installing PyTorch, torchvision, torchaudio ---\n",
            "--- Installing transformers, accelerate, bitsandbytes ---\n",
            "\n",
            "--- Checking Installed Library Versions ---\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Torchvision version: 0.20.1+cu121\n",
            "Transformers version: 4.51.3\n",
            "Accelerate version: 1.6.0\n",
            "BitsandBytes version: 0.45.5\n",
            "--- Libraries Installed ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "nZCYdp4eOmvaY9yajQBSdv2A",
      "metadata": {
        "tags": [],
        "id": "nZCYdp4eOmvaY9yajQBSdv2A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639,
          "referenced_widgets": [
            "01ca70e27af84f98b9771b241d1e40bf",
            "3368757536754f4c81dab81035241ad4",
            "0d7341df26384bcf85ab96a085198ee9",
            "2ef0124d01794741a2b926d27f2aa560",
            "2deac0354f8549deac5c0ab25b83f80e",
            "9806ac25ab8e40b791b80eb2a89e1d79",
            "a8459f718b7544d697f841f34aeb4871",
            "82389cc334e34039b94208d18817753f",
            "6abf36c9ec90420b96339ef77d5d1416",
            "6e901a3d0a4c4a6da75b0440c0cb54c9",
            "92a856f2cf3b45d1a65bd25b99958a10"
          ]
        },
        "outputId": "50b69f00-5d42-455c-8739-cd8fd6b02df6"
      },
      "source": [
        "\n",
        "# --- Cell 2: Import Libraries ---\n",
        "# (No changes needed)\n",
        "print(\"\\n--- Importing Libraries ---\")\n",
        "import torch\n",
        "import torch.nn.functional as F # For padding\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LogitsProcessorList, MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria\n",
        "import gc\n",
        "import re\n",
        "import time # For timing comparison\n",
        "import traceback # For detailed error printing\n",
        "from typing import List, Dict, Union # Added for type hinting\n",
        "import os\n",
        "import json\n",
        "import numpy\n",
        "import numpy as np\n",
        "print(\"--- Libraries Imported ---\")\n",
        "\n",
        "\n",
        "# --- Cell 3: Configuration: Model Names, Quantization, Device ---\n",
        "# (No changes needed)\n",
        "print(\"\\n--- Configuring Models and Device ---\")\n",
        "\n",
        "# --- ADD THESE ---\n",
        "\n",
        "print(f\"--- Using Fixed Task Template ---\")\n",
        "\n",
        "CHUNK_SIZE_FOR_FOCUSED_SMALL_MODELS = 128\n",
        "print(f\"Chunk size for focused M_Small: {CHUNK_SIZE_FOR_FOCUSED_SMALL_MODELS} tokens\")\n",
        "\n",
        "# --- Paths for manual run ---\n",
        "PROJECT_BASE_PATH = '/content/' # Base path in Colab\n",
        "NEEDLE_SET_HARD_PATH = os.path.join(PROJECT_BASE_PATH, \"needle_set_hard.json\")\n",
        "HAYSTACK_BOOK_PATH = os.path.join(PROJECT_BASE_PATH, \"my_book.txt\")\n",
        "\n",
        "# --- Test Parameters for this Simplified Run ---\n",
        "# CHOOSE ONE CONTEXT LENGTH TO START, e.g., 32k. Max your M_Large can handle.\n",
        "CONTEXT_LENGTHS_TO_TEST = [2048]\n",
        "DEPTH_PERCENTAGES_TO_TEST = [0.5] # Test a few depths\n",
        "ALPHA_MOD = 1 # Not used in baseline mode, but kept for completeness\n",
        "MAX_NEW_TOKENS_GENERATION = 50\n",
        "GLOBAL_SYSTEM_PROMPT_FOR_RUN = \"You are a helpful AI assistant. Use the information provided in the book snippet to answer the question. Your answer should be short and based on either explicitly stated facts or strong, logical inferences.\"\n",
        "ENABLE_QWEN3_THINKING_FOR_RUN = True # Set to False for baseline as Qwen3 thinking is often a fine-tuning feature\n",
        "\n",
        "\n",
        "# --- Specify your Qwen2/Qwen3 models ---\n",
        "model_large_name = \"Qwen/Qwen3-1.7B\" # Example: Using Qwen3 as the large model\n",
        "model_small_name = \"Qwen/Qwen3-0.6B\"   # Example: Using a smaller Qwen3 for deltas\n",
        "\n",
        "print(f\"--- Model Configuration ---\")\n",
        "print(f\"Large (M_Large):    {model_large_name}\")\n",
        "print(f\"Small (M_Small):    {model_small_name} (for ICL-based deltas)\")\n",
        "print(\"-------------------------\")\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: CUDA not available, running on CPU will be extremely slow.\")\n",
        "\n",
        "# --- Quantization Configuration (Applied to ALL models) ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "#bnb_config = BitsAndBytesConfig(\n",
        "#    load_in_8bit=True,\n",
        "#    bnb_8bit_compute_dtype=torch.bfloat16 # Keep compute_dtype for potential performance benefits\n",
        "#)\n",
        "print(\"Using 4-bit NF4 quantization with bfloat16 compute dtype for all models.\")\n",
        "print(\"--- Configuration Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 4: Load Models and Tokenizer ---\n",
        "# (MODIFIED for Qwen3 considerations)\n",
        "print(\"\\n--- Loading Tokenizer and Models ---\")\n",
        "\n",
        "# --- Load Tokenizer (Use tokenizer from the large Qwen3 model) ---\n",
        "print(\"Loading Tokenizer...\")\n",
        "try:\n",
        "    # For Qwen3's `enable_thinking`, it's crucial this tokenizer supports it.\n",
        "    # Typically, this means loading the tokenizer from the specific Qwen3 model.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_large_name, trust_remote_code=True) # Ensure model_large_name is a Qwen3 model\n",
        "\n",
        "    # Qwen3 tokenizer should have a chat_template. This check is still good practice.\n",
        "    if tokenizer.chat_template is None:\n",
        "         print(\"WARNING: Tokenizer does not have a chat_template defined. Attempting to set a default Qwen template.\")\n",
        "         # This default might need adjustment for Qwen3 if it differs significantly\n",
        "         tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{% if enable_thinking %}{{ '<|im_start|>assistant\\n<think>' }}{% else %}{{ '<|im_start|>assistant\\n' }}{% endif %}{% endif %}\"\n",
        "         print(\"Default Qwen chat template (with basic thinking tag consideration) applied. VERIFY THIS IS CORRECT FOR QWEN3.\")\n",
        "         # The above template is a guess. Qwen3's actual template might be more complex.\n",
        "         # It's best if the tokenizer.chat_template is already correctly defined by from_pretrained.\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        if tokenizer.eos_token is not None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            print(f\"Tokenizer pad_token set to eos_token ({tokenizer.eos_token}).\")\n",
        "        else:\n",
        "            print(\"WARNING: Tokenizer has no EOS token. Adding a default PAD token '<|pad|>' for Qwen3.\")\n",
        "            # Qwen3 might have a specific pad token or handle padding differently.\n",
        "            # For Qwen2 it was <|endoftext|> (ID 151643) or <|extra_0|> if you add one.\n",
        "            # For Qwen3, if eos_token is None, this needs careful checking.\n",
        "            # Let's assume eos_token will be present for Qwen3.\n",
        "            tokenizer.add_special_tokens({'pad_token': '<|pad|>'}) # A generic pad if truly needed\n",
        "    if tokenizer.pad_token_id is None:\n",
        "         raise ValueError(\"Tokenizer pad_token_id is None even after setting pad_token. Cannot proceed.\")\n",
        "    print(f\"Using PAD token ID: {tokenizer.pad_token_id}\")\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    print(f\"Tokenizer padding side set to '{tokenizer.padding_side}'.\")\n",
        "\n",
        "    # Get the ID for </think> token for parsing, if it exists\n",
        "    # The ID 151668 was for Qwen/Qwen3-30B-A3B. It might vary for other Qwen3 models.\n",
        "    # It's safer to get it dynamically if possible, or make it a constant if you know the model.\n",
        "    try:\n",
        "        think_end_token_id_qwen3 = tokenizer.encode(\"</think>\", add_special_tokens=False)[0]\n",
        "        print(f\"Qwen3 '</think>' token ID found: {think_end_token_id_qwen3}\")\n",
        "    except:\n",
        "        think_end_token_id_qwen3 = 151668 # Fallback to the example ID\n",
        "        print(f\"Warning: Could not dynamically encode '</think>'. Using fallback ID: {think_end_token_id_qwen3}. VERIFY THIS ID.\")\n",
        "\n",
        "    print(\"Tokenizer Loaded Successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading tokenizer for {model_large_name}: {e}\")\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits: torch.Tensor,\n",
        "                          top_k: int = 0,\n",
        "                          top_p: float = 0.0,\n",
        "                          filter_value: float = -float('Inf'),\n",
        "                          min_tokens_to_keep: int = 1) -> torch.Tensor:\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size, vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                         Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "            filter_value: value to assign to filtered logits.\n",
        "            min_tokens_to_keep: minimum number of tokens we must keep (default 1).\n",
        "\n",
        "        Returns:\n",
        "            Logits with filtered elements set to filter_value.\n",
        "            Shape: (batch size, vocabulary size)\n",
        "    \"\"\"\n",
        "    if top_k == 0 and top_p == 0.0:\n",
        "        return logits # No filtering needed\n",
        "\n",
        "    if logits.ndim == 1:\n",
        "        # print(\"top_k_top_p_filtering: Warning: logits is 1D, unsqueezing to add batch dimension.\") # Removed for speed\n",
        "        logits = logits.unsqueeze(0)\n",
        "\n",
        "    batch_size, vocab_size = logits.size()\n",
        "\n",
        "    if top_k > 0:\n",
        "        # Safety check: ensure top_k is not larger than vocab size\n",
        "        top_k = min(max(top_k, min_tokens_to_keep), vocab_size)\n",
        "        # Keep at least min_tokens_to_keep (default 1) tokens\n",
        "\n",
        "        # Find the top_k values and their indices for each batch item\n",
        "        topk_values, _ = torch.topk(logits, top_k, dim=-1)\n",
        "\n",
        "        # Get the k-th value for each batch item (shape: batch_size, 1)\n",
        "        kth_value = topk_values[..., -1, None]\n",
        "\n",
        "        # Create a mask for values less than the k-th value\n",
        "        indices_to_remove = logits < kth_value\n",
        "        # Apply the filter\n",
        "        logits = logits.masked_fill(indices_to_remove, filter_value)\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        # Sort logits in descending order\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
        "\n",
        "        # Calculate cumulative probabilities\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (nucleus filtering)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        # Ensure we keep at least min_tokens_to_keep tokens\n",
        "        if min_tokens_to_keep > 1:\n",
        "             sorted_indices_to_remove[..., :min_tokens_to_keep] = False\n",
        "        else:\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = False\n",
        "\n",
        "        # Create a final mask for the original logits tensor\n",
        "        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool, device=logits.device)\n",
        "        indices_to_remove.scatter_(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "\n",
        "        # Apply the filter\n",
        "        logits = logits.masked_fill(indices_to_remove, filter_value)\n",
        "\n",
        "    return logits\n",
        "# --- End of modeling.utils inclusion ---\n",
        "\n",
        "# --- Function to load model ---\n",
        "def load_model(model_name, config, device):\n",
        "    print(f\"Loading Model: {model_name}...\")\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=config,\n",
        "            device_map=\"auto\", # Handles multi-GPU and places layers optimally\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.bfloat16 # Consistent dtype\n",
        "        )\n",
        "        model.eval()\n",
        "        print(f\"{model_name} Loaded Successfully.\")\n",
        "        try:\n",
        "            mem_bytes = model.get_memory_footprint()\n",
        "            print(f\"Estimated memory footprint for {model_name}: {mem_bytes / 1e9:.2f} GB\")\n",
        "        except Exception:\n",
        "            print(\"Could not estimate memory footprint automatically.\")\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading model {model_name}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return None\n",
        "\n",
        "# --- Load Models ---\n",
        "model_large = load_model(model_large_name, bnb_config, device)\n",
        "model_small = load_model(model_small_name, bnb_config, device)\n",
        "\n",
        "if not all([model_large, model_small]):\n",
        "    # ... (error handling for model loading) ...\n",
        "    raise RuntimeError(\"One or more models failed to load. Cannot proceed. Check logs above.\")\n",
        "else:\n",
        "    print(\"\\nAll models loaded successfully with 4-bit quantization.\")\n",
        "    # ... (resize token embeddings if necessary, same logic as before) ...\n",
        "\n",
        "print(\"--- Tokenizer and Models Loaded ---\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Cell 5: Implement Mixture of Dexperts Generation (BATCHED with KV Cache) ---\n",
        "print(\"\\n--- Defining BookHaystack Class & MoD Generation ---\")\n",
        "\n",
        "# --- BookHaystack Class (simplified version) ---\n",
        "class BookHaystack:\n",
        "    def __init__(self, book_path: str, tokenizer_instance: AutoTokenizer): # Takes tokenizer instance\n",
        "        self.book_path = book_path\n",
        "        self.tokenizer = tokenizer_instance # Store it\n",
        "        if not os.path.exists(book_path):\n",
        "            raise FileNotFoundError(f\"Book path {book_path} does not exist\")\n",
        "        with open(book_path, 'r', encoding='utf-8') as f:\n",
        "            self.text = f.read()\n",
        "        self.text_encoded_full = None # Store full book encoding\n",
        "\n",
        "    def _get_book_tokens(self):\n",
        "        if self.text_encoded_full is None:\n",
        "            print(\"Encoding full book for BookHaystack (one time)...\")\n",
        "            self.text_encoded_full = self.tokenizer.encode(self.text, add_special_tokens=False)\n",
        "            print(f\"Book has {len(self.text_encoded_full)} tokens.\")\n",
        "        return self.text_encoded_full\n",
        "\n",
        "    def get_haystack_with_needle(self, needle_text: str, target_haystack_len: int, depth_percentage: float) -> str:\n",
        "        \"\"\"\n",
        "        Creates a haystack of approx. target_haystack_len tokens from the book,\n",
        "        with needle_text inserted at depth_percentage.\n",
        "        \"\"\"\n",
        "        book_tokens = self._get_book_tokens()\n",
        "        if not book_tokens: return needle_text # Should not happen if book is loaded\n",
        "\n",
        "        # 1. Determine the slice of the book to use as the base for the haystack\n",
        "        # We want the final haystack (book part + needle) to be around target_haystack_len\n",
        "        # This is a simplified approach. A more robust one might take needle length into account earlier.\n",
        "\n",
        "        # For very long books and large target_haystack_len, we might not start from token 0.\n",
        "        # For simplicity here, we'll always try to take from the beginning of the book.\n",
        "        book_tokens_for_haystack = book_tokens[:target_haystack_len]\n",
        "\n",
        "        # 2. Insert needle into this slice\n",
        "        needle_tokens = self.tokenizer.encode(\" \" + needle_text + \"\\n\", add_special_tokens=False)\n",
        "\n",
        "        insertion_point_in_slice = int(len(book_tokens_for_haystack) * depth_percentage)\n",
        "        insertion_point_in_slice = max(0, min(insertion_point_in_slice, len(book_tokens_for_haystack)))\n",
        "\n",
        "        tokens_before = book_tokens_for_haystack[:insertion_point_in_slice]\n",
        "        tokens_after = book_tokens_for_haystack[insertion_point_in_slice:]\n",
        "\n",
        "        final_haystack_tokens = tokens_before + needle_tokens + tokens_after\n",
        "\n",
        "        # Truncate if the addition of needle made it exceed target_haystack_len by too much\n",
        "        # (This is a simple safety; NoLiMa's context is usually about model input capacity)\n",
        "        if len(final_haystack_tokens) > target_haystack_len + len(needle_tokens): # Allow some overflow for needle\n",
        "             final_haystack_tokens = final_haystack_tokens[:target_haystack_len + len(needle_tokens)]\n",
        "\n",
        "        return self.tokenizer.decode(final_haystack_tokens)\n",
        "\n",
        "\n",
        "print(\"\\n--- Defining BATCHED MoD Generation Function (Qwen3 Thinking Aware) ---\")\n",
        "\n",
        "SYSTEM = \"system\"\n",
        "USER = \"user\"\n",
        "ASSISTANT = \"assistant\"\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_mod_batch_kv_cache(\n",
        "    haystacks_full_text: List[str], # MODIFIED: List of haystack strings for the batch\n",
        "    questions_text: List[str],     # MODIFIED: List of question strings for the batch\n",
        "    max_new_tokens: int = 150,\n",
        "    temperature: float = 0.6,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.0,\n",
        "    alpha: float = 1.0,\n",
        "    delta_magnitude_threshold: float = 0.0,\n",
        "    # These _in parameters are to avoid conflict if globals with same name exist\n",
        "    global_system_prompt_in: str = \"You are a helpful AI assistant.\",\n",
        "    enable_thinking_in: bool = True,\n",
        "    qwen3_think_end_token_id_in: int = 151668, # Fallback\n",
        "    chunk_size_small_in: int = 8192 # Fallback\n",
        ") -> Dict[str, Union[List[str], List[str], float, str]]:\n",
        "    global tokenizer, model_large, model_small, device # Using globals for this manual script\n",
        "\n",
        "    if not all([model_large, model_small, tokenizer]):\n",
        "        return {\"outputs\": [], \"thinking_outputs\": [], \"tokens_per_second\": 0.0, \"error\": \"Models or tokenizer not loaded.\"}\n",
        "    if len(haystacks_full_text) != len(questions_text):\n",
        "        return {\"outputs\": [], \"thinking_outputs\": [], \"tokens_per_second\": 0.0, \"error\": \"Haystacks and questions must have the same batch size.\"}\n",
        "\n",
        "\n",
        "    generation_start_time = time.time()\n",
        "    batch_size = len(haystacks_full_text)\n",
        "\n",
        "    # --- Prepare Initial Inputs ---\n",
        "\n",
        "    # 1. For M_Large (SYSTEM + FULL_HAYSTACK + QUESTION)\n",
        "    prompts_for_large_model_templated_text = []\n",
        "    for i in range(batch_size):\n",
        "        # MODIFIED: Construct combined user content for M_Large\n",
        "        # The NoLiMa template is \"Haystack: {haystack} ... Question: {question}\"\n",
        "        # We recreate this structure if haystacks_full_text[i] is the haystack with needle\n",
        "        # and questions_text[i] is the retrieval question.\n",
        "        # Note: The exact formatting might depend on how NoLiMa's original `task_template`\n",
        "        #       was structured if you were using it. For a manual run, this is a direct way.\n",
        "        #questions_text[i] += '<think>\\n'\n",
        "        if 'You can only think for 20 words, then give a one word answer.' not in questions_text[i]:\n",
        "          questions_text[i] += 'You can only think for 20 words, then give a one word answer.\\n<think>'\n",
        "\n",
        "        #combined_user_content_for_large = \"Question: \" + questions_text[i]\n",
        "        combined_user_content_for_large = haystacks_full_text[i] + \"\\n\\nQuestion: \" + questions_text[i]\n",
        "\n",
        "        messages_large = [\n",
        "            {\"role\": \"system\", \"content\": global_system_prompt_in},\n",
        "            {\"role\": \"user\", \"content\": combined_user_content_for_large}\n",
        "        ]\n",
        "        prompts_for_large_model_templated_text.append(tokenizer.apply_chat_template(\n",
        "            messages_large, add_generation_prompt=True, tokenize=False, enable_thinking=enable_thinking_in\n",
        "        ))\n",
        "    initial_inputs_large = tokenizer(prompts_for_large_model_templated_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=model_large.config.max_position_embeddings - 10).to(device)\n",
        "    prompt_lengths_large = [torch.sum(mask).item() for mask in initial_inputs_large[\"attention_mask\"]] # Used for decoding later\n",
        "\n",
        "    # 2. For M_Small_unprimed (SYSTEM + QUESTION_ONLY)\n",
        "    prompts_for_small_unprimed_templated_text = []\n",
        "    for i in range(batch_size):\n",
        "        # MODIFIED: User content is ONLY the question\n",
        "        messages_small_unprimed = [\n",
        "            {\"role\": \"system\", \"content\": global_system_prompt_in},\n",
        "            {\"role\": \"user\", \"content\": questions_text[i]}\n",
        "        ]\n",
        "        prompts_for_small_unprimed_templated_text.append(tokenizer.apply_chat_template(\n",
        "            messages_small_unprimed, add_generation_prompt=True, tokenize=False, enable_thinking=enable_thinking_in\n",
        "        ))\n",
        "    initial_inputs_small_unprimed = tokenizer(prompts_for_small_unprimed_templated_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=model_small.config.max_position_embeddings - 10).to(device)\n",
        "\n",
        "    # 3. For M_Small_focused_chunk_i (SYSTEM + CHUNK_i_OF_HAYSTACK + QUESTION)\n",
        "    #    MODIFIED: This whole section is new/adapted from previous chunking logic\n",
        "    temp_chunks_plus_question_for_prompts = [[] for _ in range(batch_size)]\n",
        "    max_num_chunks_across_batch = 0\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        current_haystack_text = haystacks_full_text[i] # This is haystack_with_needle\n",
        "        current_question_text = questions_text[i]\n",
        "\n",
        "        haystack_only_tokens = tokenizer.encode(current_haystack_text, add_special_tokens=False)\n",
        "        num_chunks_for_this_prompt = (len(haystack_only_tokens) + chunk_size_small_in - 1) // chunk_size_small_in\n",
        "        if num_chunks_for_this_prompt == 0 and len(haystack_only_tokens) > 0: num_chunks_for_this_prompt = 1\n",
        "        elif len(haystack_only_tokens) == 0: num_chunks_for_this_prompt = 0\n",
        "        max_num_chunks_across_batch = max(max_num_chunks_across_batch, num_chunks_for_this_prompt)\n",
        "\n",
        "        for chunk_idx in range(num_chunks_for_this_prompt):\n",
        "            start_token_idx = chunk_idx * chunk_size_small_in\n",
        "            end_token_idx = min((chunk_idx + 1) * chunk_size_small_in, len(haystack_only_tokens))\n",
        "            current_haystack_chunk_token_ids = haystack_only_tokens[start_token_idx:end_token_idx]\n",
        "            current_haystack_chunk_str = tokenizer.decode(current_haystack_chunk_token_ids)\n",
        "\n",
        "            combined_user_content_for_focused = current_haystack_chunk_str + \"\\n\\nQuestion: \" + current_question_text\n",
        "            chunk_plus_question_messages = [\n",
        "                {\"role\": \"system\", \"content\": global_system_prompt_in},\n",
        "                {\"role\": \"user\", \"content\": combined_user_content_for_focused}\n",
        "            ]\n",
        "            templated_prompt = tokenizer.apply_chat_template(\n",
        "                chunk_plus_question_messages, add_generation_prompt=True, tokenize=False, enable_thinking=enable_thinking_in\n",
        "            )\n",
        "            tokenized_data = tokenizer(\n",
        "                templated_prompt, return_tensors=\"pt\", truncation=True,\n",
        "                max_length=model_small.config.max_position_embeddings - 10\n",
        "            )\n",
        "            temp_chunks_plus_question_for_prompts[i].append({\n",
        "                \"input_ids\": tokenized_data.input_ids.squeeze(0),\n",
        "                \"attention_mask\": tokenized_data.attention_mask.squeeze(0)\n",
        "            })\n",
        "\n",
        "    initial_inputs_small_focused_list = []\n",
        "    if max_num_chunks_across_batch > 0:\n",
        "        for chunk_k_idx in range(max_num_chunks_across_batch):\n",
        "            batch_input_ids_k, batch_attn_masks_k = [], []\n",
        "            max_len_k = 0\n",
        "            for prompt_idx in range(batch_size):\n",
        "                if chunk_k_idx < len(temp_chunks_plus_question_for_prompts[prompt_idx]):\n",
        "                    ids = temp_chunks_plus_question_for_prompts[prompt_idx][chunk_k_idx][\"input_ids\"]\n",
        "                    mask = temp_chunks_plus_question_for_prompts[prompt_idx][chunk_k_idx][\"attention_mask\"]\n",
        "                    batch_input_ids_k.append(ids)\n",
        "                    batch_attn_masks_k.append(mask)\n",
        "                    max_len_k = max(max_len_k, ids.size(0))\n",
        "\n",
        "            padded_ids_list_k, padded_masks_list_k = [], []\n",
        "            for idx_in_batch in range(batch_size): # Iterate up to batch_size to ensure all spots are filled\n",
        "                if chunk_k_idx < len(temp_chunks_plus_question_for_prompts[idx_in_batch]): # If this prompt had this chunk\n",
        "                    ids = temp_chunks_plus_question_for_prompts[idx_in_batch][chunk_k_idx][\"input_ids\"]\n",
        "                    mask = temp_chunks_plus_question_for_prompts[idx_in_batch][chunk_k_idx][\"attention_mask\"]\n",
        "                    pad_len = max_len_k - ids.size(0)\n",
        "                    padded_ids_list_k.append(F.pad(ids, (pad_len, 0), value=tokenizer.pad_token_id))\n",
        "                    padded_masks_list_k.append(F.pad(mask, (pad_len, 0), value=0))\n",
        "                else: # This prompt was shorter than k chunks, add full padding\n",
        "                    padded_ids_list_k.append(torch.full((max_len_k,), tokenizer.pad_token_id, dtype=torch.long, device=device))\n",
        "                    padded_masks_list_k.append(torch.zeros((max_len_k,), dtype=torch.long, device=device))\n",
        "\n",
        "            if padded_ids_list_k: # Should always be true if max_num_chunks > 0\n",
        "                 initial_inputs_small_focused_list.append({\n",
        "                    \"input_ids\": torch.stack(padded_ids_list_k).to(device),\n",
        "                    \"attention_mask\": torch.stack(padded_masks_list_k).to(device)\n",
        "                })\n",
        "    num_focused_experts = len(initial_inputs_small_focused_list)\n",
        "\n",
        "    # --- KV Cache Init --- (Same as your original MoD script, adapted names)\n",
        "    past_key_values_large = None\n",
        "    past_key_values_small_unprimed = None\n",
        "    past_key_values_small_focused_list = [None] * num_focused_experts # MODIFIED: for focused experts\n",
        "\n",
        "    # --- Termination Setup --- (Same as your original MoD script)\n",
        "    stop_token_ids_set = set()\n",
        "    try:\n",
        "        im_end_token_list = tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)\n",
        "        if im_end_token_list: stop_token_ids_set.add(im_end_token_list[0])\n",
        "        if tokenizer.eos_token_id is not None and tokenizer.eos_token_id not in stop_token_ids_set:\n",
        "            stop_token_ids_set.add(tokenizer.eos_token_id)\n",
        "    except Exception as e: print(f\"Warning: Error getting termination token IDs: {e}.\")\n",
        "    if not stop_token_ids_set and tokenizer.eos_token_id is not None: stop_token_ids_set = {tokenizer.eos_token_id}\n",
        "    if not stop_token_ids_set: print(\"CRITICAL WARNING: No EOS token ID found for stop criteria.\")\n",
        "    else: print(f\"Termination Token IDs: {stop_token_ids_set}\")\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    if pad_token_id is None: raise ValueError(\"tokenizer.pad_token_id is None.\")\n",
        "\n",
        "\n",
        "    # --- Vocab Size Handling --- (Same as your original MoD script)\n",
        "    vocab_size_large = model_large.config.vocab_size\n",
        "    vocab_size_small = model_small.config.vocab_size\n",
        "    max_vocab_size = max(vocab_size_large, vocab_size_small)\n",
        "    needs_padding_large = vocab_size_large < max_vocab_size\n",
        "    needs_padding_small = vocab_size_small < max_vocab_size\n",
        "    pad_value_logits = float('-inf')\n",
        "\n",
        "    # --- Initialize generated_ids and attention masks for the loop ---\n",
        "    generated_ids = initial_inputs_large[\"input_ids\"].clone() # M_Large drives the sequence length\n",
        "    current_attention_mask_large = initial_inputs_large[\"attention_mask\"].clone()\n",
        "    current_attention_mask_small_unprimed = initial_inputs_small_unprimed[\"attention_mask\"].clone() # MODIFIED: for unprimed\n",
        "    current_attention_masks_small_focused = [] # MODIFIED: for focused\n",
        "    if num_focused_experts > 0:\n",
        "        current_attention_masks_small_focused = [item[\"attention_mask\"].clone() for item in initial_inputs_small_focused_list]\n",
        "\n",
        "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "    total_new_tokens_generated = 0\n",
        "    # --- ADD THESE ---\n",
        "    loop_start_time = time.time()\n",
        "    last_print_time = loop_start_time\n",
        "    # --- END ADD ---\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        # --- ADD THIS ---\n",
        "        step_start_time = time.time()\n",
        "        # --- END ADD ---\n",
        "\n",
        "        if step == 0:\n",
        "            input_ids_large_step = initial_inputs_large[\"input_ids\"]\n",
        "            input_ids_small_unprimed_step = initial_inputs_small_unprimed[\"input_ids\"] # MODIFIED\n",
        "            if num_focused_experts > 0:\n",
        "                input_ids_small_focused_step_list = [item[\"input_ids\"] for item in initial_inputs_small_focused_list] # MODIFIED\n",
        "        else:\n",
        "            input_ids_large_step = next_token_id # next_token_id is from combined logits\n",
        "            input_ids_small_unprimed_step = next_token_id # MODIFIED\n",
        "            if num_focused_experts > 0:\n",
        "                input_ids_small_focused_step_list = [next_token_id] * num_focused_experts # MODIFIED\n",
        "\n",
        "        try:\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.bfloat16):\n",
        "                # 1. M_Large Forward Pass\n",
        "                outputs_large = model_large(\n",
        "                    input_ids=input_ids_large_step,\n",
        "                    attention_mask=current_attention_mask_large,\n",
        "                    past_key_values=past_key_values_large,\n",
        "                    use_cache=True\n",
        "                )\n",
        "                logits_large_raw = outputs_large.logits[:, -1, :]\n",
        "                past_key_values_large = outputs_large.past_key_values\n",
        "                logits_large_aligned = F.pad(logits_large_raw, (0, max_vocab_size - logits_large_raw.shape[-1]), value=pad_value_logits) if needs_padding_large else logits_large_raw\n",
        "\n",
        "                average_expert_delta = torch.zeros_like(logits_large_aligned)\n",
        "\n",
        "                # MODIFIED: Logic for unprimed and focused experts\n",
        "                if num_focused_experts > 0: # Only proceed if there are chunks to process\n",
        "                    # 2. M_Small (Unprimed, Question-Only) Forward Pass\n",
        "                    outputs_small_unprimed = model_small(\n",
        "                        input_ids=input_ids_small_unprimed_step,\n",
        "                        attention_mask=current_attention_mask_small_unprimed,\n",
        "                        past_key_values=past_key_values_small_unprimed,\n",
        "                        use_cache=True\n",
        "                    )\n",
        "                    logits_small_unprimed_raw = outputs_small_unprimed.logits[:, -1, :]\n",
        "                    past_key_values_small_unprimed = outputs_small_unprimed.past_key_values\n",
        "                    logits_small_unprimed_aligned = F.pad(logits_small_unprimed_raw, (0, max_vocab_size - logits_small_unprimed_raw.shape[-1]), value=pad_value_logits) if needs_padding_small else logits_small_unprimed_raw\n",
        "\n",
        "                    # 3. M_Small (Focused on Chunks + Question) Forward Passes\n",
        "                    all_processed_deltas = [] # Store deltas after potential filtering\n",
        "\n",
        "                    for i in range(num_focused_experts):\n",
        "                        outputs_small_focused = model_small(\n",
        "                            input_ids=input_ids_small_focused_step_list[i],\n",
        "                            attention_mask=current_attention_masks_small_focused[i],\n",
        "                            past_key_values=past_key_values_small_focused_list[i],\n",
        "                            use_cache=True\n",
        "                        )\n",
        "                        logits_small_focused_i_raw = outputs_small_focused.logits[:, -1, :]\n",
        "                        past_key_values_small_focused_list[i] = outputs_small_focused.past_key_values\n",
        "                        logits_small_focused_i_aligned = F.pad(logits_small_focused_i_raw, (0, max_vocab_size - logits_small_focused_i_raw.shape[-1]), value=pad_value_logits) if needs_padding_small else logits_small_focused_i_raw\n",
        "\n",
        "                        delta_i = logits_small_focused_i_aligned - logits_small_unprimed_aligned # Delta uses the new unprimed baseline\n",
        "                        # --- PRINT DELTA MAGNITUDES (ONLY FOR FIRST STEP) ---\n",
        "                        if False:\n",
        "                            abs_delta_values = torch.abs(delta_i)\n",
        "                            print(f\"  Shape of delta_i for this expert batch: {delta_i.shape}\")\n",
        "                            print(f\"  Max magnitude (across batch & vocab): {abs_delta_values.max().item():.4f}\")\n",
        "                            print(f\"  Mean magnitude (across batch & vocab): {abs_delta_values.mean().item():.4f}\")\n",
        "                            print(f\"  Median magnitude (across batch & vocab): {torch.median(abs_delta_values).item():.4f}\")\n",
        "                            flat_abs_deltas = abs_delta_values.flatten()\n",
        "                            if flat_abs_deltas.numel() > 0:\n",
        "                                percentiles_to_calc = torch.tensor([0.50, 0.75, 0.90, 0.95, 0.99, 0.999], device=flat_abs_deltas.device)\n",
        "                                quantiles = torch.quantile(flat_abs_deltas.float(), percentiles_to_calc)\n",
        "                                for p_idx_loop, p_val_tensor in enumerate(percentiles_to_calc): # Renamed p_idx to p_idx_loop\n",
        "                                    p_val = p_val_tensor.item()\n",
        "                                    q_val = quantiles[p_idx_loop].item() # Renamed p_idx to p_idx_loop\n",
        "                                    print(f\"  {p_val*100:.1f}th percentile magnitude: {q_val:.4f}\")\n",
        "                        # --- END PRINT DELTA MAGNITUDES ---\n",
        "\n",
        "\n",
        "                        if delta_magnitude_threshold > 0.0:\n",
        "                              # Create a mask where the absolute delta is greater than the threshold\n",
        "                              significant_delta_mask = torch.abs(delta_i) > delta_magnitude_threshold\n",
        "                              # Apply the mask: keep delta_i where mask is true, else 0\n",
        "                              processed_delta_i = torch.where(significant_delta_mask, delta_i, torch.zeros_like(delta_i))\n",
        "                        else: # No thresholding (or threshold is zero), use the original delta\n",
        "                            processed_delta_i = delta_i\n",
        "                        all_processed_deltas.append(processed_delta_i)\n",
        "\n",
        "                    if all_processed_deltas: # If list is not empty\n",
        "                        average_expert_delta = torch.stack(all_processed_deltas).mean(dim=0)\n",
        "                        # If all processed_delta_i were zero tensors, average_expert_delta will correctly be a zero tensor.\n",
        "\n",
        "                modified_logits = logits_large_aligned + alpha * average_expert_delta\n",
        "\n",
        "        # --- The rest of the loop is largely the same as your original Cell 5 ---\n",
        "        # (Error handling, NaN/Inf check, Sampling, Update generated_ids and attention masks, Termination Criteria, Print Progress)\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError as oom_e:\n",
        "            # ... (your OOM handling) ...\n",
        "            # MODIFIED: Use qwen3_think_end_token_id_in and enable_thinking_in\n",
        "            decoded_results_dict = decode_batch_results_qwen3_thinking(generated_ids, prompt_lengths_large, tokenizer, stop_token_ids_set, pad_token_id, enable_thinking_in, qwen3_think_end_token_id_in)\n",
        "            return {\"outputs\": decoded_results_dict[\"outputs\"], \"thinking_outputs\": decoded_results_dict[\"thinking_outputs\"], \"tokens_per_second\": 0.0, \"error\": str(oom_e)}\n",
        "        except Exception as e:\n",
        "            # ... (your general exception handling) ...\n",
        "            decoded_results_dict = decode_batch_results_qwen3_thinking(generated_ids, prompt_lengths_large, tokenizer, stop_token_ids_set, pad_token_id, enable_thinking_in, qwen3_think_end_token_id_in)\n",
        "            return {\"outputs\": decoded_results_dict[\"outputs\"], \"thinking_outputs\": decoded_results_dict[\"thinking_outputs\"], \"tokens_per_second\": 0.0, \"error\": str(e)}\n",
        "\n",
        "        # --- Check for and Handle NaN/Inf in modified_logits ---\n",
        "        if torch.isnan(modified_logits).any() or torch.isinf(modified_logits).any():\n",
        "            nan_inf_mask = torch.isnan(modified_logits) | torch.isinf(modified_logits)\n",
        "            if ((step + 1) % 10 == 0 or step == 0) and nan_inf_mask.any():\n",
        "                print(f\"Warning: NaN/Inf detected in combined logits at step {step+1} for {nan_inf_mask.sum().item()} elements. Replacing NaN with -inf.\")\n",
        "            modified_logits = torch.nan_to_num(modified_logits, nan=float('-inf'), posinf=float('inf'), neginf=float('-inf'))\n",
        "        # --- Sampling ---\n",
        "        # (The sampling block from your original code seems fine, so it's used here)\n",
        "        if temperature > 0:\n",
        "            scaled_logits = modified_logits / temperature\n",
        "            filtered_logits = top_k_top_p_filtering(scaled_logits, top_k=top_k, top_p=top_p)\n",
        "            probabilities = F.softmax(filtered_logits, dim=-1)\n",
        "            nan_probs_mask = torch.isnan(probabilities).any(dim=-1)\n",
        "\n",
        "            if nan_probs_mask.any():\n",
        "                # print(f\"Warning: NaN in probabilities at step {step+1} for {nan_probs_mask.sum().item()} sequences. Falling back.\") # Less verbose\n",
        "                fallback_k = min(top_k if top_k > 0 else 5, scaled_logits.size(-1)) # Fallback to top_k or 5\n",
        "                if fallback_k == 0: fallback_k = 1\n",
        "\n",
        "                _, top_k_indices_fallback = torch.topk(scaled_logits[nan_probs_mask], k=fallback_k, dim=-1)\n",
        "                uniform_probs_topk = torch.ones_like(top_k_indices_fallback, dtype=torch.float) / fallback_k\n",
        "                uniform_sampled_relative_indices = torch.multinomial(uniform_probs_topk, num_samples=1)\n",
        "                uniform_next_token_id = torch.gather(top_k_indices_fallback, dim=-1, index=uniform_sampled_relative_indices)\n",
        "\n",
        "                next_token_id = torch.full((batch_size, 1), pad_token_id, dtype=torch.long, device=device)\n",
        "                normal_probs_mask = ~nan_probs_mask\n",
        "                if normal_probs_mask.any():\n",
        "                    normal_probs = probabilities[normal_probs_mask]\n",
        "                    normal_probs = torch.clamp(normal_probs, min=0.0)\n",
        "                    normal_probs_sum = normal_probs.sum(dim=-1, keepdim=True)\n",
        "                    # Avoid division by zero if sum is zero (all filtered out)\n",
        "                    safe_sum = torch.where(normal_probs_sum == 0, torch.ones_like(normal_probs_sum), normal_probs_sum)\n",
        "                    normal_probs = normal_probs / safe_sum\n",
        "                    normal_probs = torch.where(torch.isnan(normal_probs), torch.ones_like(normal_probs) / normal_probs.size(-1), normal_probs)\n",
        "\n",
        "                    if normal_probs.numel() > 0 and not torch.isnan(normal_probs).all(): # Check if multinomial can run\n",
        "                         normal_next_token_id = torch.multinomial(normal_probs, num_samples=1)\n",
        "                         next_token_id[normal_probs_mask] = normal_next_token_id\n",
        "                    else: # All normal_probs were NaN or empty, use fallback for these too (e.g. greedy)\n",
        "                        # print(f\"Warning: Normal probs also problematic for {normal_probs_mask.sum().item()} sequences. Using greedy on scaled_logits.\")\n",
        "                        greedy_fallback_tokens = torch.argmax(scaled_logits[normal_probs_mask], dim=-1).unsqueeze(-1)\n",
        "                        next_token_id[normal_probs_mask] = greedy_fallback_tokens\n",
        "\n",
        "\n",
        "                next_token_id[nan_probs_mask] = uniform_next_token_id\n",
        "                if (next_token_id == pad_token_id).all() and batch_size > 0 :\n",
        "                    # print(f\"Error/Warning: All sequences failed sampling at step {step+1}. Using greedy from modified_logits.\")\n",
        "                    next_token_id = torch.argmax(modified_logits, dim=-1).unsqueeze(-1) # Last resort\n",
        "            else:\n",
        "                probabilities = torch.clamp(probabilities, min=0.0)\n",
        "                probs_sum = probabilities.sum(dim=-1, keepdim=True)\n",
        "                safe_sum = torch.where(probs_sum == 0, torch.ones_like(probs_sum), probs_sum)\n",
        "                probabilities = probabilities / safe_sum\n",
        "                probabilities = torch.where(torch.isnan(probabilities), torch.ones_like(probabilities) / probabilities.size(-1), probabilities)\n",
        "                if probabilities.numel() > 0 and not torch.isnan(probabilities).all():\n",
        "                    next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
        "                else:\n",
        "                    # print(f\"Warning: All probabilities problematic at step {step+1}. Using greedy from modified_logits.\")\n",
        "                    next_token_id = torch.argmax(modified_logits, dim=-1).unsqueeze(-1) # Last resort\n",
        "\n",
        "        else: # Greedy (temperature == 0)\n",
        "            next_token_id = torch.argmax(modified_logits, dim=-1).unsqueeze(-1)        # next_token_id = ...\n",
        "\n",
        "        # --- Stop finished sequences from generating further ---\n",
        "        # Mask next_token_id with pad_token_id for sequences that are already finished\n",
        "        next_token_id = next_token_id * unfinished_sequences.unsqueeze(-1) + \\\n",
        "                        pad_token_id * (1 - unfinished_sequences.unsqueeze(-1))\n",
        "        # (Update generated sequences and attention masks for NEXT iteration - same structure)\n",
        "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "        new_token_attention = unfinished_sequences.unsqueeze(-1)\n",
        "        current_attention_mask_large = torch.cat([current_attention_mask_large, new_token_attention], dim=-1)\n",
        "        current_attention_mask_small_unprimed = torch.cat([current_attention_mask_small_unprimed, new_token_attention], dim=-1) # MODIFIED\n",
        "        if num_focused_experts > 0:\n",
        "            for i in range(num_focused_experts):\n",
        "                current_attention_masks_small_focused[i] = torch.cat([current_attention_masks_small_focused[i], new_token_attention], dim=-1) # MODIFIED\n",
        "\n",
        "        # --- Check Termination Criteria ---\n",
        "        current_token_scalars = next_token_id.squeeze(-1)\n",
        "        hit_stop_token = torch.isin(current_token_scalars, torch.tensor(list(stop_token_ids_set), device=device))\n",
        "\n",
        "        # Increment token count only for sequences that were active (unfinished or just finished)\n",
        "        tokens_generated_this_step_count = unfinished_sequences.sum().item()\n",
        "        total_new_tokens_generated += tokens_generated_this_step_count\n",
        "\n",
        "        # --- Print Progress ---\n",
        "        current_time = time.time()\n",
        "        if current_time - last_print_time >= 10.0 or (step == max_new_tokens - 1) or (unfinished_sequences.max() == 0 and step > 0):\n",
        "             active_sequences = unfinished_sequences.sum().item()\n",
        "             print(f\"  Step {step+1}/{max_new_tokens} | Active: {active_sequences}/{batch_size} | Last tokens: {current_token_scalars[:min(5, batch_size)].tolist()} | Step time: {current_time - step_start_time:.3f}s\")\n",
        "             last_print_time = current_time\n",
        "\n",
        "        if unfinished_sequences.max() == 0:\n",
        "            print(f\"All sequences finished generating at step {step+1}.\")\n",
        "            break\n",
        "        # (Break if all sequences finished - same as your script)\n",
        "\n",
        "    # --- Post-Loop ---\n",
        "    print(\"Generation loop finished.\")\n",
        "    loop_end_time = time.time()\n",
        "    total_loop_time = loop_end_time - loop_start_time\n",
        "\n",
        "    print(f\"Total generation time (loop): {total_loop_time:.2f} seconds for {total_new_tokens_generated} total new tokens.\")\n",
        "    tokens_per_second = 0.0\n",
        "    if total_new_tokens_generated > 0 and total_loop_time > 0.01: # Avoid division by zero for very fast runs\n",
        "        tokens_per_second = total_new_tokens_generated / total_loop_time\n",
        "        print(f\"Aggregate speed: {tokens_per_second:.2f} tokens/second.\")\n",
        "    elif total_new_tokens_generated == 0:\n",
        "        print(\"No new tokens were generated.\")\n",
        "\n",
        "    decoded_results_dict = decode_batch_results_qwen3_thinking(\n",
        "        generated_ids, prompt_lengths_large, tokenizer, stop_token_ids_set,\n",
        "        pad_token_id, enable_thinking_in, qwen3_think_end_token_id_in # Use the _in suffixed parameters\n",
        "    )\n",
        "\n",
        "    generation_end_time = time.time()\n",
        "    print(f\"Total function execution time: {generation_end_time - generation_start_time:.2f} seconds.\")\n",
        "    print(\"--- BATCH MoD Generation Function (Qwen3 Thinking Aware) Complete ---\")\n",
        "    return {\n",
        "        \"outputs\": decoded_results_dict[\"outputs\"],\n",
        "        \"thinking_outputs\": decoded_results_dict[\"thinking_outputs\"],\n",
        "        \"tokens_per_second\": tokens_per_second,\n",
        "        \"error\": None # No error if successful completion\n",
        "    }\n",
        "\n",
        "\n",
        "def decode_batch_results_qwen3_thinking(\n",
        "    generated_ids: torch.Tensor,\n",
        "    prompt_lengths: List[int],\n",
        "    tokenizer,\n",
        "    stop_token_ids_set: set,\n",
        "    pad_token_id: int,\n",
        "    is_thinking_enabled: bool,\n",
        "    think_end_token_id: int\n",
        ") -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Helper function to decode batch results, handling Qwen3-style thinking content.\n",
        "    Returns a dictionary with 'outputs' and 'thinking_outputs'.\n",
        "    \"\"\"\n",
        "    batch_outputs = []\n",
        "    batch_thinking_outputs = []\n",
        "    batch_size = generated_ids.shape[0]\n",
        "    # stop_token_strings = {tokenizer.decode(stop_id) for stop_id in stop_token_ids_set if stop_id != pad_token_id} # Not directly used in Qwen3 parsing example\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = prompt_lengths[i]\n",
        "        response_ids_with_pad = generated_ids[i, prompt_len:]\n",
        "        response_ids_list = response_ids_with_pad[response_ids_with_pad != pad_token_id].tolist()\n",
        "\n",
        "        thinking_content = \"\"\n",
        "        main_content = \"\"\n",
        "\n",
        "        if is_thinking_enabled:\n",
        "            try:\n",
        "                # Find the last occurrence of </think> token ID\n",
        "                # rindex finding think_end_token_id (e.g., 151668 for </think>)\n",
        "                idx_think_end = len(response_ids_list) - 1 - response_ids_list[::-1].index(think_end_token_id)\n",
        "                # The thinking content is from the start of the response up to and including </think>\n",
        "                # The Qwen example decodes up to BEFORE </think> for thinking_content,\n",
        "                # and FROM </think> for main_content. Let's follow that.\n",
        "                # The index returned by .index() is 0-based from the *reversed* list.\n",
        "                # So, if </think> is the last token, rev_idx = 0. len - 1 - 0 = actual last index.\n",
        "                # We want tokens *before* </think> for thinking, and tokens *from* </think> for content.\n",
        "                # Let's adjust `idx_think_end` to be the index of the </think> token itself.\n",
        "\n",
        "                # Corrected logic based on example:\n",
        "                # index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "                # This `index` in the example is the position *after* the </think> tag in the `output_ids` *slice*.\n",
        "                # It means: `output_ids[:index]` is thinking (inclusive of </think>), `output_ids[index:]` is content.\n",
        "                # However, the example then decodes thinking_content up to `output_ids[:index]`\n",
        "                # and content from `output_ids[index:]`.\n",
        "                # The example code has:\n",
        "                # thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)\n",
        "                # content = tokenizer.decode(output_ids[index:], skip_special_tokens=True)\n",
        "                # This implies `index` is the split point. If `</think>` is at `k`, `output_ids[:k]` for thinking,\n",
        "                # and `output_ids[k+1:]` for content. Or rather, what their code implies is\n",
        "                # `output_ids[:index_of_think_end_tag]` is thinking, and `output_ids[index_of_think_end_tag:]` is content that starts with </think>\n",
        "                # which is then skipped by `skip_special_tokens=True`.\n",
        "\n",
        "                # Let's re-interpret the example's `index` logic:\n",
        "                # `output_ids[::-1].index(think_end_token_id)` gives num elements *after* last `think_end_token_id` in reversed list\n",
        "                # `split_point_after_think = len(response_ids_list) - response_ids_list[::-1].index(think_end_token_id)`\n",
        "                # This `split_point_after_think` is the index in `response_ids_list` that is ONE AFTER the `</think>` token.\n",
        "                # Example: [1,2,THINK_END,4,5]. rev=[5,4,THINK_END,2,1]. rev.index(THINK_END)=2. len=5. 5-2=3.\n",
        "                # response_ids_list[:3] = [1,2,THINK_END]. response_ids_list[3:] = [4,5]\n",
        "                # This seems correct for the example's decoding slices.\n",
        "\n",
        "                split_point_after_think = len(response_ids_list) - response_ids_list[::-1].index(think_end_token_id)\n",
        "\n",
        "                thinking_ids_part = response_ids_list[:split_point_after_think]\n",
        "                content_ids_part = response_ids_list[split_point_after_think:]\n",
        "\n",
        "                # The original Qwen example's decode for thinking_content includes the </think> tag,\n",
        "                # but skip_special_tokens=True might remove it if it's registered as special.\n",
        "                # Let's decode up to *before* </think> for \"thinking\" and from *after* for \"content\"\n",
        "                # to be cleaner, if skip_special_tokens doesn't remove </think>.\n",
        "                # If </think> (151668) is special, then `tokenizer.decode(thinking_ids_part, skip_special_tokens=True)`\n",
        "                # would give thinking_content, and `tokenizer.decode(content_ids_part, skip_special_tokens=True)` gives content.\n",
        "                # This seems to be the intent of the Qwen example.\n",
        "\n",
        "                thinking_content = tokenizer.decode(thinking_ids_part, skip_special_tokens=True).strip()\n",
        "                main_content = tokenizer.decode(content_ids_part, skip_special_tokens=True).strip()\n",
        "\n",
        "            except ValueError: # think_end_token_id not found\n",
        "                # No </think> tag found, assume all is main content\n",
        "                print(f\"Warning: Qwen3 </think> token ID {think_end_token_id} not found in response for item {i}, though thinking was enabled. Treating all as content.\")\n",
        "                main_content = tokenizer.decode(response_ids_list, skip_special_tokens=True).strip()\n",
        "                thinking_content = \"\" # No thinking content if tag not found\n",
        "        else: # Thinking not enabled, all response is main content\n",
        "            main_content = tokenizer.decode(response_ids_list, skip_special_tokens=True).strip()\n",
        "            thinking_content = \"\"\n",
        "\n",
        "\n",
        "        # Final cleanup of other stop tokens from main_content if necessary\n",
        "        # (Original script had this, might still be useful)\n",
        "        # for stop_str in stop_token_strings:\n",
        "        #     if stop_str and main_content.endswith(stop_str):\n",
        "        #         main_content = main_content[:-len(stop_str)].rstrip()\n",
        "\n",
        "        batch_outputs.append(main_content)\n",
        "        batch_thinking_outputs.append(thinking_content)\n",
        "\n",
        "    return {\"outputs\": batch_outputs, \"thinking_outputs\": batch_thinking_outputs}\n",
        "\n",
        "\n",
        "def decode_batch_results(generated_ids: torch.Tensor,\n",
        "                         prompt_lengths: List[int],\n",
        "                         tokenizer,\n",
        "                         stop_token_ids_set: set,\n",
        "                         pad_token_id: int,\n",
        "                         include_think_prompt: bool) -> List[str]:\n",
        "    \"\"\"Helper function to decode the batch results and clean them up.\"\"\"\n",
        "    decoded_outputs = []\n",
        "    batch_size = generated_ids.shape[0]\n",
        "    stop_token_strings = {tokenizer.decode(stop_id) for stop_id in stop_token_ids_set if stop_id != pad_token_id}\n",
        "    think_end_tag = \"</think>\"\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = prompt_lengths[i]\n",
        "        response_ids = generated_ids[i, prompt_len:]\n",
        "\n",
        "        # Filter out padding tokens\n",
        "        actual_response_ids = response_ids[response_ids != pad_token_id]\n",
        "\n",
        "        response_text = tokenizer.decode(actual_response_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Clean up potential trailing tags/tokens\n",
        "        if include_think_prompt and response_text.strip().endswith(think_end_tag):\n",
        "            response_text = response_text.rsplit(think_end_tag, 1)[0].strip()\n",
        "            # print(f\"Cleaned trailing '{think_end_tag}' tag for sequence {i}.\") # Optional debug print\n",
        "\n",
        "        # Final cleanup of stop tokens that might linger\n",
        "        cleaned = False\n",
        "        for stop_str in stop_token_strings:\n",
        "            if stop_str and response_text.endswith(stop_str):\n",
        "                response_text = response_text[:-len(stop_str)].rstrip()\n",
        "                cleaned = True\n",
        "        #if cleaned: print(f\"Cleaned trailing stop token for sequence {i}.\") # Optional debug print\n",
        "\n",
        "        decoded_outputs.append(response_text.strip())\n",
        "\n",
        "    return decoded_outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Importing Libraries ---\n",
            "--- Libraries Imported ---\n",
            "\n",
            "--- Configuring Models and Device ---\n",
            "--- Using Fixed Task Template ---\n",
            "Chunk size for focused M_Small: 128 tokens\n",
            "--- Model Configuration ---\n",
            "Large (M_Large):    Qwen/Qwen3-1.7B\n",
            "Small (M_Small):    Qwen/Qwen3-0.6B (for ICL-based deltas)\n",
            "-------------------------\n",
            "Using device: cuda\n",
            "Using 4-bit NF4 quantization with bfloat16 compute dtype for all models.\n",
            "--- Configuration Complete ---\n",
            "\n",
            "--- Loading Tokenizer and Models ---\n",
            "Loading Tokenizer...\n",
            "Using PAD token ID: 151643\n",
            "Tokenizer padding side set to 'left'.\n",
            "Qwen3 '</think>' token ID found: 151668\n",
            "Tokenizer Loaded Successfully.\n",
            "Loading Model: Qwen/Qwen3-1.7B...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01ca70e27af84f98b9771b241d1e40bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen/Qwen3-1.7B Loaded Successfully.\n",
            "Estimated memory footprint for Qwen/Qwen3-1.7B: 1.33 GB\n",
            "Loading Model: Qwen/Qwen3-0.6B...\n",
            "Qwen/Qwen3-0.6B Loaded Successfully.\n",
            "Estimated memory footprint for Qwen/Qwen3-0.6B: 0.53 GB\n",
            "\n",
            "All models loaded successfully with 4-bit quantization.\n",
            "--- Tokenizer and Models Loaded ---\n",
            "\n",
            "--- Defining BookHaystack Class & MoD Generation ---\n",
            "\n",
            "--- Defining BATCHED MoD Generation Function (Qwen3 Thinking Aware) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Parameters for this Simplified Run ---\n",
        "# CHOOSE ONE CONTEXT LENGTH TO START, e.g., 32k. Max your M_Large can handle.\n",
        "CONTEXT_LENGTHS_TO_TEST = [2048]\n",
        "DEPTH_PERCENTAGES_TO_TEST = [0.5] # Test a few depths\n",
        "ALPHA_MOD = 1\n",
        "MAX_NEW_TOKENS_GENERATION = 100\n",
        "GLOBAL_SYSTEM_PROMPT_FOR_RUN = \"You are a helpful AI assistant. Use the information provided in the book snippet to answer the question. Your answer should be short and based on either explicitly stated facts or strong, logical inferences.\"\n",
        "ENABLE_QWEN3_THINKING_FOR_RUN = True # Set to False for baseline as Qwen3 thinking is often a fine-tuning feature\n",
        "\n",
        "\n",
        "\n",
        "# --- Cell 6: Manual NOLIMA-Hard Evaluation Loop ---\n",
        "print(\"\\n--- Starting Manual NOLIMA-Hard Evaluation ---\")\n",
        "\n",
        "# Ensure models and tokenizer are loaded from Cell 4\n",
        "if tokenizer is None or model_large is None or model_small is None:\n",
        "    print(\"ERROR: Models or tokenizer not loaded. Please run Cell 4 first.\")\n",
        "else:\n",
        "    # Load NOLIMA-Hard dataset\n",
        "    nolima_hard_configs_loaded = []\n",
        "    if os.path.exists(NEEDLE_SET_HARD_PATH):\n",
        "        with open(NEEDLE_SET_HARD_PATH, \"r\") as f_json:\n",
        "            nolima_hard_configs_loaded = json.load(f_json)\n",
        "        print(f\"Loaded {len(nolima_hard_configs_loaded)} experiment configurations from NOLIMA-Hard set.\")\n",
        "    else:\n",
        "        print(f\"ERROR: NOLIMA-Hard file not found at {NEEDLE_SET_HARD_PATH}\")\n",
        "\n",
        "    # Instantiate BookHaystack\n",
        "    try:\n",
        "        book_processor = BookHaystack(HAYSTACK_BOOK_PATH, tokenizer_instance=tokenizer)\n",
        "        print(f\"Loaded haystack content from: {HAYSTACK_BOOK_PATH}\")\n",
        "    except Exception as e_book:\n",
        "        print(f\"Error loading haystack: {e_book}\")\n",
        "        book_processor = None\n",
        "\n",
        "    all_run_results = []\n",
        "\n",
        "    if nolima_hard_configs_loaded and book_processor:\n",
        "        for current_ctx_len in CONTEXT_LENGTHS_TO_TEST:\n",
        "            print(f\"\\n===== EVALUATING FOR TARGET HAYSTACK CONTEXT LENGTH: {current_ctx_len} tokens =====\")\n",
        "\n",
        "            for exp_conf in nolima_hard_configs_loaded[3:4]:\n",
        "                exp_id_val = exp_conf[\"id\"]\n",
        "\n",
        "                for q_type, q_template in exp_conf[\"questions\"].items():\n",
        "                    for t_id, t_details in exp_conf[\"tests\"].items():\n",
        "\n",
        "                        needle_template_val = exp_conf[\"needle\"]\n",
        "                        input_args_val = t_details[\"input_args\"]\n",
        "                        gold_answers_val = t_details.get(\"gold_answers\", \"\")\n",
        "                        char_set_val = exp_conf.get(\"character_set\", [])\n",
        "\n",
        "                        # --- Substitute CHAR and args ---\n",
        "                        final_needle = needle_template_val\n",
        "                        final_question = q_template\n",
        "                        actual_selected_char = None # Initialize actual_selected_char\n",
        "\n",
        "                        if \"{CHAR}\" in needle_template_val or \"{CHAR}\" in q_template: # Check original templates\n",
        "                            if not char_set_val:\n",
        "                                print(f\"WARNING: {{CHAR}} in template but no character_set for {exp_id_val}_{t_id}_{q_type}. Skipping this test case.\")\n",
        "                                # Optionally, log this skipped case or handle it differently\n",
        "                                all_run_results.append({\n",
        "                                    \"exp_id\": exp_id_val, \"test_id\": t_id, \"q_type\": q_type,\n",
        "                                    \"haystack_target_len\": current_ctx_len, \"depth\": depth_val, # Assuming depth_val is available here or loop it\n",
        "                                    \"needle\": needle_template_val, \"question\": q_template,\n",
        "                                    \"char_selected\": None, \"gold\": gold_answers_val, # gold_answers_val would be \"\"\n",
        "                                    \"answer\": \"SKIPPED_NO_CHAR_SET\", \"score\": 0,\n",
        "                                    \"error\": \"Character set missing for {CHAR} template.\"\n",
        "                                })\n",
        "                                continue # Skip to the next test case iteration\n",
        "\n",
        "                            actual_selected_char = np.random.choice(char_set_val)\n",
        "                            final_needle = final_needle.replace(\"{CHAR}\", actual_selected_char)\n",
        "                            final_question = final_question.replace(\"{CHAR}\", actual_selected_char)\n",
        "\n",
        "                            # --- KEY CHANGE FOR GOLD ANSWER ---\n",
        "                            # If a character was selected, this IS the gold answer for scoring.\n",
        "                            # We make it a list to be consistent with how scoring might handle multiple gold answers.\n",
        "                            gold_answers_val = [actual_selected_char]\n",
        "                            # --- END KEY CHANGE ---\n",
        "\n",
        "                        # Argument substitution should happen AFTER {CHAR} substitution\n",
        "                        for arg_i, arg_v in enumerate(input_args_val):\n",
        "                            placeholder_str = \"{\" + str(arg_i + 1) + \"}\"\n",
        "                            final_needle = final_needle.replace(placeholder_str, str(arg_v))\n",
        "                            final_question = final_question.replace(placeholder_str, str(arg_v))\n",
        "\n",
        "                            # If gold_answers_val was set from {CHAR}, it's now a list like ['selected_char_name']\n",
        "                            # If it was loaded from JSON and was a list/string, substitute args there too.\n",
        "                            if isinstance(gold_answers_val, list):\n",
        "                                gold_answers_val = [\n",
        "                                    ans.replace(placeholder_str, str(arg_v)) if isinstance(ans, str) else ans\n",
        "                                    for ans in gold_answers_val\n",
        "                                ]\n",
        "                            elif isinstance(gold_answers_val, str): # Only if not set by {CHAR}\n",
        "                                gold_answers_val = gold_answers_val.replace(placeholder_str, str(arg_v))\n",
        "                        # --- End Substitution ---\n",
        "\n",
        "\n",
        "                        for arg_i, arg_v in enumerate(input_args_val):\n",
        "                            placeholder_str = \"{\" + str(arg_i + 1) + \"}\" # Assuming {1}, {2} are argument placeholders\n",
        "                            final_needle = final_needle.replace(placeholder_str, str(arg_v)) # Ensure arg_v is string\n",
        "                            final_question = final_question.replace(placeholder_str, str(arg_v)) # Ensure arg_v is string\n",
        "                        # --- End Substitution ---\n",
        "\n",
        "                        print(f\"\\n--- Running: {exp_id_val}_{t_id}_{q_type} (Haystack len: {current_ctx_len}) ---\")\n",
        "                        print(f\"  Needle: {final_needle[:70]}...\")\n",
        "                        print(f\"  Question: {final_question}\")\n",
        "\n",
        "                        for depth_val in DEPTH_PERCENTAGES_TO_TEST:\n",
        "                            print(f\"  Depth: {depth_val*100:.0f}%\")\n",
        "\n",
        "                            # 1. Create haystack with needle\n",
        "                            # `current_ctx_len` is the target length of the book snippet part\n",
        "                            haystack_text_with_needle = book_processor.get_haystack_with_needle(\n",
        "                                needle_text=final_needle,\n",
        "                                target_haystack_len=current_ctx_len,\n",
        "                                depth_percentage=depth_val\n",
        "                            )\n",
        "\n",
        "                            # 2. Call your MoD generation\n",
        "                            # (Pass global constants from Cell 3 as arguments here)\n",
        "                            eval_output = generate_mod_batch_kv_cache(\n",
        "                                haystacks_full_text=[haystack_text_with_needle], # Batch of 1\n",
        "                                questions_text=[final_question],             # Batch of 1\n",
        "                                max_new_tokens=MAX_NEW_TOKENS_GENERATION,\n",
        "                                temperature=0.0, # Or from config\n",
        "                                top_k=0,        # Or from config\n",
        "                                top_p=0.95,      # Or from config\n",
        "                                alpha=ALPHA_MOD,\n",
        "                                delta_magnitude_threshold=0.0,\n",
        "                                global_system_prompt_in=GLOBAL_SYSTEM_PROMPT_FOR_RUN,\n",
        "                                enable_thinking_in=ENABLE_QWEN3_THINKING_FOR_RUN,\n",
        "                                qwen3_think_end_token_id_in=think_end_token_id_qwen3,\n",
        "                                chunk_size_small_in=CHUNK_SIZE_FOR_FOCUSED_SMALL_MODELS\n",
        "                            )\n",
        "\n",
        "                            ans_text = eval_output[\"outputs\"][0] if eval_output[\"outputs\"] and not eval_output.get(\"error\") else \"ERROR_IN_GENERATION\"\n",
        "                            print(f\"    Model Answer: {ans_text[:70]}...\")\n",
        "\n",
        "                            # 3. Score\n",
        "                            current_score = 0\n",
        "                            if isinstance(gold_answers_val, list):\n",
        "                                if any(g.lower() in ans_text.lower() for g in gold_answers_val if g): current_score = 1\n",
        "                            elif isinstance(gold_answers_val, str) and gold_answers_val:\n",
        "                                if gold_answers_val.lower() in ans_text.lower(): current_score = 1\n",
        "                            print(f\"    Score: {current_score}\")\n",
        "\n",
        "                            all_run_results.append({\n",
        "                                \"exp_id\": exp_id_val, \"test_id\": t_id, \"q_type\": q_type,\n",
        "                                \"haystack_target_len\": current_ctx_len, \"depth\": depth_val,\n",
        "                                \"needle\": final_needle, \"question\": final_question,\n",
        "                                \"char_selected\": actual_selected_char, \"gold\": gold_answers_val,\n",
        "                                \"answer\": ans_text, \"score\": current_score,\n",
        "                                \"tps\": eval_output.get(\"tokens_per_second\"),\n",
        "                                \"error\": eval_output.get(\"error\")\n",
        "                            })\n",
        "                            gc.collect()\n",
        "                            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "        # --- Save results after each context length or at the end ---\n",
        "        results_filename_temp = f\"manual_nolima_hard_results_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(os.path.join(PROJECT_BASE_PATH, results_filename_temp), \"w\") as f_out_json:\n",
        "            json.dump(all_run_results, f_out_json, indent=2)\n",
        "        print(f\"Intermediate results saved to {results_filename_temp}\")\n",
        "\n",
        "    print(\"\\n--- Manual NOLIMA-Hard Evaluation Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hR4mrrQiRlo",
        "outputId": "6d16cd2a-e582-41eb-8a4e-65fd75549fff"
      },
      "id": "3hR4mrrQiRlo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Manual NOLIMA-Hard Evaluation ---\n",
            "Loaded 4 experiment configurations from NOLIMA-Hard set.\n",
            "Loaded haystack content from: /content/my_book.txt\n",
            "\n",
            "===== EVALUATING FOR TARGET HAYSTACK CONTEXT LENGTH: 2048 tokens =====\n",
            "\n",
            "--- Running: 0409Inv_T10_C02_onehop (Haystack len: 2048) ---\n",
            "  Needle: There was an engineer living in Calvinia, named Diana....\n",
            "  Question: Which character has been to South Africa?\n",
            "  Depth: 50%\n",
            "Encoding full book for BookHaystack (one time)...\n",
            "Book has 98060 tokens.\n",
            "Termination Token IDs: {151645}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-fb54ab69328a>:504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.bfloat16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step 7/100 | Active: 1/1 | Last tokens: [1430] | Step time: 1.483s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcWioN3iiZl8"
      },
      "id": "IcWioN3iiZl8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "qwen3",
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01ca70e27af84f98b9771b241d1e40bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3368757536754f4c81dab81035241ad4",
              "IPY_MODEL_0d7341df26384bcf85ab96a085198ee9",
              "IPY_MODEL_2ef0124d01794741a2b926d27f2aa560"
            ],
            "layout": "IPY_MODEL_2deac0354f8549deac5c0ab25b83f80e"
          }
        },
        "3368757536754f4c81dab81035241ad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9806ac25ab8e40b791b80eb2a89e1d79",
            "placeholder": "​",
            "style": "IPY_MODEL_a8459f718b7544d697f841f34aeb4871",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0d7341df26384bcf85ab96a085198ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82389cc334e34039b94208d18817753f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6abf36c9ec90420b96339ef77d5d1416",
            "value": 2
          }
        },
        "2ef0124d01794741a2b926d27f2aa560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e901a3d0a4c4a6da75b0440c0cb54c9",
            "placeholder": "​",
            "style": "IPY_MODEL_92a856f2cf3b45d1a65bd25b99958a10",
            "value": " 2/2 [00:05&lt;00:00,  2.24s/it]"
          }
        },
        "2deac0354f8549deac5c0ab25b83f80e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9806ac25ab8e40b791b80eb2a89e1d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8459f718b7544d697f841f34aeb4871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82389cc334e34039b94208d18817753f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abf36c9ec90420b96339ef77d5d1416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e901a3d0a4c4a6da75b0440c0cb54c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92a856f2cf3b45d1a65bd25b99958a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}