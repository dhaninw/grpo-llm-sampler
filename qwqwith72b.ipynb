{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cUlcEzMiL80",
        "outputId": "cf2cf9a8-e6f2-484e-d1c9-436b36f7d6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.0/363.4 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title 1. Setup: Install Libraries\n",
        "# (Keep as is)\n",
        "!pip install transformers torch accelerate bitsandbytes sentencepiece -q\n",
        "!pip install -U bitsandbytes # Ensure latest bitsandbytes\n",
        "\n",
        "# @title 2. Import Libraries\n",
        "# (Keep as is)\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LogitsProcessorList, MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria\n",
        "import gc\n",
        "import re\n",
        "import time # For timing comparison\n",
        "\n",
        "# @title 3. Configuration: Model Names, Quantization, Device\n",
        "# (Keep as is)\n",
        "# --- Specify your Qwen2 models ---\n",
        "model_base_name = \"Qwen/Qwen2.5-72B\"\n",
        "model_expert_name = \"Qwen/QwQ-32B\"\n",
        "model_anti_expert_name = \"Qwen/Qwen2.5-32b\"\n",
        "\n",
        "print(f\"--- Model Configuration ---\")\n",
        "print(f\"Base (M):         {model_base_name}\")\n",
        "print(f\"Expert (M+):      {model_expert_name}\")\n",
        "print(f\"Anti-Expert (M-): {model_anti_expert_name}\")\n",
        "print(\"-------------------------\")\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: CUDA not available, running on CPU will be extremely slow.\")\n",
        "\n",
        "# --- Quantization Configuration (Applied to ALL models) ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "print(\"Using 4-bit NF4 quantization for all models.\")\n",
        "\n",
        "# @title 4. Load Models and Tokenizer\n",
        "\n",
        "# --- Load Tokenizer (Use tokenizer from the base model) ---\n",
        "print(\"Loading Tokenizer...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_base_name, trust_remote_code=True)\n",
        "\n",
        "    # Check if a chat template is defined\n",
        "    if tokenizer.chat_template is None:\n",
        "         print(\"WARNING: Tokenizer does not have a chat_template defined. Using default (or potentially incorrect) formatting.\")\n",
        "         # Example: tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Tokenizer pad_token set to eos_token ({tokenizer.eos_token}).\")\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    print(f\"Tokenizer padding side set to '{tokenizer.padding_side}'.\")\n",
        "\n",
        "    print(\"Tokenizer Loaded. Chat template likely available via tokenizer.apply_chat_template.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading tokenizer for {model_base_name}: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Function to load model (Keep as is) ---\n",
        "def load_model(model_name, config, device):\n",
        "    print(f\"Loading Model: {model_name}...\")\n",
        "    try:\n",
        "        # using device_map='auto' already sends layers to GPU/CPU as needed\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=config,\n",
        "            device_map=\"auto\", # Automatically distributes layers\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        model.eval() # Set to evaluation mode\n",
        "        print(f\"{model_name} Loaded Successfully.\")\n",
        "        mem_bytes = model.get_memory_footprint()\n",
        "        print(f\"Estimated memory footprint for {model_name}: {mem_bytes / 1e9:.2f} GB\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading model {model_name}: {e}\")\n",
        "        gc.collect(); torch.cuda.empty_cache(); return None\n",
        "\n",
        "# --- Load Models (Keep as is) ---\n",
        "model_base = load_model(model_base_name, bnb_config, device)\n",
        "model_expert = load_model(model_expert_name, bnb_config, device)\n",
        "model_anti_expert = load_model(model_anti_expert_name, bnb_config, device)\n",
        "\n",
        "if not all([model_base, model_expert, model_anti_expert]):\n",
        "     raise RuntimeError(\"One or more models failed to load. Cannot proceed.\")\n",
        "else:\n",
        "    print(\"\\nAll models loaded successfully with 4-bit quantization.\")\n",
        "\n",
        "\n",
        "# @title 5. Implement Proxy-Tuning Generation (Using KV Cache)\n",
        "\n",
        "# Constants for roles (optional, makes code readable)\n",
        "SYSTEM = \"system\"\n",
        "USER = \"user\"\n",
        "ASSISTANT = \"assistant\"\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_proxy_tuned_kv_cache(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 150,\n",
        "    temperature: float = 0.6,\n",
        "    top_k: int = 50,\n",
        "    alpha: float = 1.0,\n",
        "    is_math_problem: bool = False,\n",
        "    include_think_prompt: bool = True,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Generates text using proxy-tuning with KV caching for speed.\n",
        "    \"\"\"\n",
        "    if not all([model_base, model_expert, model_anti_expert]):\n",
        "        print(\"Error: Models not loaded.\"); return \"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Prepare Messages for Chat Template ---\n",
        "    system_prompt = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
        "    user_content = prompt\n",
        "    if is_math_problem and \"reason step by step\" not in prompt.lower():\n",
        "         user_content += \"\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": SYSTEM, \"content\": system_prompt},\n",
        "        {\"role\": USER, \"content\": user_content}\n",
        "    ]\n",
        "\n",
        "    # --- Apply Chat Template to get Input IDs ---\n",
        "    try:\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device) # Move initial IDs to device\n",
        "\n",
        "        # --- Add DeepSeek-R1 <think> prompt tokens if requested ---\n",
        "        if include_think_prompt:\n",
        "            think_prompt = \"<think>\\n\"\n",
        "            think_tokens = tokenizer.encode(think_prompt, add_special_tokens=False, return_tensors=\"pt\").to(input_ids.device) # Ensure on same device\n",
        "            input_ids = torch.cat([input_ids, think_tokens], dim=-1)\n",
        "\n",
        "        prompt_length = input_ids.shape[1]\n",
        "\n",
        "        print(\"--- Final Input String (Decoded from Tokens) ---\")\n",
        "        print(tokenizer.decode(input_ids[0]))\n",
        "        print(\"------------------------------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error applying chat template or adding think prompt: {e}\")\n",
        "        print(\"Ensure the tokenizer has a valid chat_template attribute.\")\n",
        "        return \"\"\n",
        "\n",
        "    # --- KV Cache Initialization ---\n",
        "    past_key_values_base = None\n",
        "    past_key_values_expert = None\n",
        "    past_key_values_anti_expert = None\n",
        "\n",
        "    # --- Termination Setup ---\n",
        "    # Get token ID for <|im_end|>\n",
        "    try:\n",
        "        im_end_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
        "        stop_token_ids = [tokenizer.eos_token_id, im_end_token_id]\n",
        "        if None in stop_token_ids: # Handle case where eos_token_id might be None\n",
        "             stop_token_ids = [tid for tid in stop_token_ids if tid is not None]\n",
        "        print(f\"Termination Token IDs: {stop_token_ids}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not get <|im_end|> token id: {e}. Using only EOS.\")\n",
        "        stop_token_ids = [tokenizer.eos_token_id]\n",
        "\n",
        "\n",
        "    generated_ids = input_ids.clone()\n",
        "    current_token_ids = input_ids # Start with the full prompt for the first pass\n",
        "\n",
        "    print(\"Starting generation with KV Cache...\")\n",
        "    for step in range(max_new_tokens):\n",
        "        step_start_time = time.time()\n",
        "        try:\n",
        "            # --- Model Forward Passes with KV Cache ---\n",
        "            # Only the *first* pass uses the full `current_token_ids`.\n",
        "            # Subsequent passes use only the last generated token and the cache.\n",
        "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "                # Base Model\n",
        "                outputs_base = model_base(\n",
        "                    current_token_ids,\n",
        "                    past_key_values=past_key_values_base,\n",
        "                    use_cache=True # Explicitly enable cache usage\n",
        "                )\n",
        "                logits_base = outputs_base.logits[:, -1, :] # Logits for the next token\n",
        "                past_key_values_base = outputs_base.past_key_values # Get updated cache\n",
        "\n",
        "                # Expert Model\n",
        "                outputs_expert = model_expert(\n",
        "                    current_token_ids,\n",
        "                    past_key_values=past_key_values_expert,\n",
        "                    use_cache=True\n",
        "                )\n",
        "                logits_expert = outputs_expert.logits[:, -1, :]\n",
        "                past_key_values_expert = outputs_expert.past_key_values\n",
        "\n",
        "                # Anti-Expert Model\n",
        "                outputs_anti_expert = model_anti_expert(\n",
        "                    current_token_ids,\n",
        "                    past_key_values=past_key_values_anti_expert,\n",
        "                    use_cache=True\n",
        "                )\n",
        "                logits_anti_expert = outputs_anti_expert.logits[:, -1, :]\n",
        "                past_key_values_anti_expert = outputs_anti_expert.past_key_values\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "             print(f\"CUDA OOM at step {step+1} (PromptLen: {prompt_length}, NewTokens: {step}). Stopping.\")\n",
        "             gc.collect(); torch.cuda.empty_cache(); break\n",
        "        except Exception as e:\n",
        "            print(f\"Inference error at step {step+1}: {e}\"); break\n",
        "\n",
        "        # --- Combine Logits (Proxy-Tuning logic) ---\n",
        "        logit_difference = logits_expert - logits_anti_expert\n",
        "        modified_logits = logits_base + alpha * logit_difference\n",
        "\n",
        "        if torch.isnan(modified_logits).any() or torch.isinf(modified_logits).any():\n",
        "            print(f\"Warning: NaN/Inf in logits at step {step+1}. Using base logits.\");\n",
        "            modified_logits = logits_base.clone() # Fallback\n",
        "\n",
        "        # --- Sampling ---\n",
        "        if temperature > 0:\n",
        "            scaled_logits = modified_logits / temperature\n",
        "            # Apply top-k filtering\n",
        "            top_k_values, top_k_indices = torch.topk(scaled_logits, min(top_k, scaled_logits.size(-1)))\n",
        "            # Create a mask tensor filled with -inf\n",
        "            filtered_logits = torch.full_like(scaled_logits, float('-inf'))\n",
        "            # Scatter the top-k values back into the mask\n",
        "            filtered_logits.scatter_(-1, top_k_indices, top_k_values)\n",
        "            # Calculate probabilities\n",
        "            probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
        "            # Handle potential NaN in probabilities after softmax (though less likely after filtering)\n",
        "            if torch.isnan(probabilities).any():\n",
        "                 print(f\"Warning: NaN in probs at step {step+1}. Uniform sample from top-k.\");\n",
        "                 # Sample uniformly from the top-k indices if probabilities are NaN\n",
        "                 next_token_idx = torch.randint(0, top_k_indices.shape[1], (1,), device=device)\n",
        "                 next_token_id = top_k_indices[:, next_token_idx]\n",
        "            else:\n",
        "                 # Sample from the filtered probability distribution\n",
        "                 next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
        "        else:\n",
        "            # Greedy decoding (select the token with the highest logit)\n",
        "            next_token_id = torch.argmax(modified_logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "        # --- Update generated sequence ---\n",
        "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "\n",
        "        # --- Prepare input for the *next* iteration ---\n",
        "        # Input for the next step is *only* the newly generated token ID\n",
        "        current_token_ids = next_token_id # Shape will be (batch_size, 1)\n",
        "\n",
        "        # --- Check for termination ---\n",
        "        if next_token_id.item() in stop_token_ids:\n",
        "            print(f\"Termination token generated (ID: {next_token_id.item()}) at step {step+1}.\")\n",
        "            break\n",
        "\n",
        "        if step % 20 == 0 and step > 0: # Print progress periodically\n",
        "             step_end_time = time.time()\n",
        "             print(f\"Step {step+1}/{max_new_tokens} ({step_end_time - step_start_time:.2f}s/step) | Seq Len: {generated_ids.shape[1]}\")\n",
        "\n",
        "    print(\"Generation finished.\")\n",
        "    end_time = time.time()\n",
        "    total_gen_tokens = generated_ids.shape[1] - prompt_length\n",
        "    print(f\"Total generation time: {end_time - start_time:.2f} seconds for {total_gen_tokens} tokens.\")\n",
        "    if total_gen_tokens > 0:\n",
        "        print(f\"Average speed: {total_gen_tokens / (end_time - start_time):.2f} tokens/second.\")\n",
        "\n",
        "\n",
        "    # --- Decode the response, excluding the prompt ---\n",
        "    response_ids = generated_ids[:, prompt_length:]\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # --- Clean up potential trailing markers ---\n",
        "    if response_text.strip().endswith(\"</think>\"):\n",
        "       response_text = response_text.rsplit(\"</think>\", 1)[0].strip()\n",
        "\n",
        "    return response_text.strip()\n",
        "\n",
        "\n",
        "# @title 6. Run Example Generation (with KV Cache)\n",
        "prompt = \"Determine all positive integers $n$ for which there exist positive integers $a$, $b$, and $c$ satisfying \\[2a^n + 3b^n = 4c^n.\\]\"\n",
        "print(f\"Original User Prompt: {prompt}\")\n",
        "\n",
        "# --- Run Proxy-Tuned Generation with KV Cache ---\n",
        "if 'model_base' in locals() and model_base is not None:\n",
        "    generated_output_kv = generate_proxy_tuned_kv_cache(\n",
        "        prompt,\n",
        "        max_new_tokens=500, # Reduced for faster example, increase if needed\n",
        "        alpha=1.0,\n",
        "        temperature=0.6,\n",
        "        top_k=50,\n",
        "        is_math_problem=True,\n",
        "        include_think_prompt=True\n",
        "    )\n",
        "    print(\"\\n--- Proxy-Tuned Output (Using KV Cache + R1 Hints) ---\")\n",
        "    print(generated_output_kv)\n",
        "else:\n",
        "    print(\"Models not loaded correctly. Please check the loading logs.\")\n",
        "\n",
        "\n",
        "# @title 7. Qwen2 / DeepSeek R1 Documentation Resources\n",
        "# (Keep documentation links as before)\n",
        "print(\"\\nRelevant documentation links provided in comments and text cell above.\")\n",
        "\n",
        "# Optional: Clean up GPU memory if you're done\n",
        "# print(\"Cleaning up models...\")\n",
        "# del model_base, model_expert, model_anti_expert\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "# print(\"Models deleted and cache cleared.\")"
      ]
    }
  ]
}