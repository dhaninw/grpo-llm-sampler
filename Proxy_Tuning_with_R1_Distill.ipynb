{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RCOBO8R7vtn",
        "outputId": "efb58ac4-8027-440c-809f-77d6a498e131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "--- Model Configuration ---\n",
            "Base (M):         Qwen/Qwen2.5-1.5B\n",
            "Expert (M+):      deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Anti-Expert (M-): Qwen/Qwen2.5-Math-1.5B\n",
            "-------------------------\n",
            "Using device: cuda\n",
            "Using 4-bit NF4 quantization for all models.\n",
            "Loading Tokenizer...\n",
            "Tokenizer padding side set to 'left'.\n",
            "Tokenizer Loaded. Chat template likely available via tokenizer.apply_chat_template.\n",
            "Loading Model: Qwen/Qwen2.5-1.5B...\n",
            "Qwen/Qwen2.5-1.5B Loaded Successfully.\n",
            "Estimated memory footprint for Qwen/Qwen2.5-1.5B: 1.12 GB\n",
            "Loading Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B Loaded Successfully.\n",
            "Estimated memory footprint for deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B: 1.59 GB\n",
            "Loading Model: Qwen/Qwen2.5-Math-1.5B...\n",
            "Qwen/Qwen2.5-Math-1.5B Loaded Successfully.\n",
            "Estimated memory footprint for Qwen/Qwen2.5-Math-1.5B: 1.12 GB\n",
            "\n",
            "All models loaded successfully with 4-bit quantization.\n",
            "Original User Prompt: Determine all positive integers $n$ for which there exist positive integers $a$, $b$, and $c$ satisfying \\[2a^n + 3b^n = 4c^n.\\]\n",
            "--- Final Input String (Decoded from Tokens) ---\n",
            "<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Determine all positive integers $n$ for which there exist positive integers $a$, $b$, and $c$ satisfying \\[2a^n + 3b^n = 4c^n.\\]\n",
            "Please reason step by step, and put your final answer within \\boxed{}.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<think>\n",
            "\n",
            "------------------------------------------------\n",
            "Starting generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-581bdf747634>:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Setup: Install Libraries\n",
        "# (Keep as is)\n",
        "!pip install transformers torch accelerate bitsandbytes sentencepiece -q\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "# @title 2. Import Libraries\n",
        "# (Keep as is)\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# @title 3. Configuration: Model Names, Quantization, Device\n",
        "# (Keep as is)\n",
        "# --- Specify your Qwen2 models ---\n",
        "model_base_name = \"Qwen/Qwen2.5-1.5B\"\n",
        "model_expert_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "model_anti_expert_name = \"Qwen/Qwen2.5-Math-1.5B\"\n",
        "\n",
        "print(f\"--- Model Configuration ---\")\n",
        "print(f\"Base (M):         {model_base_name}\")\n",
        "print(f\"Expert (M+):      {model_expert_name}\")\n",
        "print(f\"Anti-Expert (M-): {model_anti_expert_name}\")\n",
        "print(\"-------------------------\")\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: CUDA not available, running on CPU will be extremely slow.\")\n",
        "\n",
        "# --- Quantization Configuration (Applied to ALL models) ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "print(\"Using 4-bit NF4 quantization for all models.\")\n",
        "\n",
        "# @title 4. Load Models and Tokenizer\n",
        "\n",
        "# --- Load Tokenizer (Use tokenizer from the base model) ---\n",
        "print(\"Loading Tokenizer...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_base_name, trust_remote_code=True)\n",
        "\n",
        "    # Check if a chat template is defined\n",
        "    if tokenizer.chat_template is None:\n",
        "         print(\"WARNING: Tokenizer does not have a chat_template defined. Using default (or potentially incorrect) formatting.\")\n",
        "         # Optionally set a default template here if needed, but Qwen should have one.\n",
        "         # Example: tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Tokenizer pad_token set to eos_token ({tokenizer.eos_token}).\")\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    print(f\"Tokenizer padding side set to '{tokenizer.padding_side}'.\")\n",
        "\n",
        "    print(\"Tokenizer Loaded. Chat template likely available via tokenizer.apply_chat_template.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading tokenizer for {model_base_name}: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Function to load model (Keep as is) ---\n",
        "def load_model(model_name, config, device):\n",
        "    print(f\"Loading Model: {model_name}...\")\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        model.eval()\n",
        "        print(f\"{model_name} Loaded Successfully.\")\n",
        "        mem_bytes = model.get_memory_footprint()\n",
        "        print(f\"Estimated memory footprint for {model_name}: {mem_bytes / 1e9:.2f} GB\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading model {model_name}: {e}\")\n",
        "        gc.collect(); torch.cuda.empty_cache(); return None\n",
        "\n",
        "# --- Load Models (Keep as is) ---\n",
        "model_base = load_model(model_base_name, bnb_config, device)\n",
        "model_expert = load_model(model_expert_name, bnb_config, device)\n",
        "model_anti_expert = load_model(model_anti_expert_name, bnb_config, device)\n",
        "\n",
        "if not all([model_base, model_expert, model_anti_expert]):\n",
        "     raise RuntimeError(\"One or more models failed to load. Cannot proceed.\")\n",
        "else:\n",
        "    print(\"\\nAll models loaded successfully with 4-bit quantization.\")\n",
        "\n",
        "\n",
        "# @title 5. Implement Proxy-Tuning Generation (Using tokenizer.apply_chat_template)\n",
        "\n",
        "# Constants for roles (optional, makes code readable)\n",
        "SYSTEM = \"system\"\n",
        "USER = \"user\"\n",
        "ASSISTANT = \"assistant\"\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_proxy_tuned(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 150,\n",
        "    temperature: float = 0.6, # DeepSeek R1 recommended default\n",
        "    top_k: int = 50,\n",
        "    alpha: float = 1.0,\n",
        "    is_math_problem: bool = False,\n",
        "    include_think_prompt: bool = True # Control adding <think> prompt for R1\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Generates text using proxy-tuning, formatting input via tokenizer.apply_chat_template\n",
        "    and optionally adding DeepSeek-R1 hints.\n",
        "    \"\"\"\n",
        "    if not all([model_base, model_expert, model_anti_expert]):\n",
        "        print(\"Error: Models not loaded.\"); return \"\"\n",
        "\n",
        "    # --- Prepare Messages for Chat Template ---\n",
        "    system_prompt = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
        "    user_content = prompt\n",
        "    if is_math_problem and \"reason step by step\" not in prompt.lower():\n",
        "         user_content += \"\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": SYSTEM, \"content\": system_prompt},\n",
        "        {\"role\": USER, \"content\": user_content}\n",
        "    ]\n",
        "\n",
        "    # --- Apply Chat Template to get Input IDs ---\n",
        "    try:\n",
        "        # Get token IDs for the conversation history + the assistant generation prompt\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True, # Important: Adds the prompt for the assistant to start talking (e.g., <|im_start|>assistant\\n)\n",
        "            tokenize=True,              # Return tensor of token IDs\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        # --- Add DeepSeek-R1 <think> prompt tokens if requested ---\n",
        "        if include_think_prompt:\n",
        "            think_prompt = \"<think>\\n\"\n",
        "            # Tokenize the think prompt *without* adding special BOS/EOS tokens\n",
        "            think_tokens = tokenizer.encode(think_prompt, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
        "            # Concatenate the think tokens to the end of the templated input\n",
        "            input_ids = torch.cat([input_ids, think_tokens], dim=-1)\n",
        "\n",
        "        # Store the length of the full prompt (template + think tokens) for later decoding\n",
        "        prompt_length = input_ids.shape[1]\n",
        "\n",
        "        # --- Optional: Decode for verification ---\n",
        "        print(\"--- Final Input String (Decoded from Tokens) ---\")\n",
        "        print(tokenizer.decode(input_ids[0]))\n",
        "        print(\"------------------------------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error applying chat template or adding think prompt: {e}\")\n",
        "        print(\"Ensure the tokenizer has a valid chat_template attribute.\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "    # --- Generation Loop (mostly unchanged) ---\n",
        "    generated_ids = input_ids.clone()\n",
        "    print(\"Starting generation...\")\n",
        "    for step in range(max_new_tokens):\n",
        "        current_input_ids = generated_ids\n",
        "\n",
        "        try:\n",
        "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "                logits_base = model_base(current_input_ids).logits[:, -1, :]\n",
        "                logits_expert = model_expert(current_input_ids).logits[:, -1, :]\n",
        "                logits_anti_expert = model_anti_expert(current_input_ids).logits[:, -1, :]\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "             print(f\"CUDA OOM at step {step+1} (SeqLen: {current_input_ids.shape[1]}). Stopping.\")\n",
        "             gc.collect(); torch.cuda.empty_cache(); break\n",
        "        except Exception as e:\n",
        "            print(f\"Inference error at step {step+1}: {e}\"); break\n",
        "\n",
        "        logit_difference = logits_expert - logits_anti_expert\n",
        "        modified_logits = logits_base + alpha * logit_difference\n",
        "\n",
        "        if torch.isnan(modified_logits).any() or torch.isinf(modified_logits).any():\n",
        "            print(f\"Warning: NaN/Inf in logits at step {step+1}. Using base logits.\");\n",
        "            modified_logits = logits_base.clone()\n",
        "\n",
        "        if temperature > 0:\n",
        "            scaled_logits = modified_logits / temperature\n",
        "            top_k_values, top_k_indices = torch.topk(scaled_logits, min(top_k, scaled_logits.size(-1)))\n",
        "            filtered_logits = torch.full_like(scaled_logits, float('-inf'))\n",
        "            filtered_logits.scatter_(-1, top_k_indices, top_k_values)\n",
        "            probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
        "            if torch.isnan(probabilities).any():\n",
        "                 print(f\"Warning: NaN in probs at step {step+1}. Uniform sample from top-k.\");\n",
        "                 next_token_id = top_k_indices[:, torch.randint(0, top_k_indices.shape[1], (1,)).item()].unsqueeze(-1)\n",
        "            else:\n",
        "                 next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
        "        else:\n",
        "             next_token_id = torch.argmax(modified_logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "\n",
        "        # Check for termination tokens (EOS or Qwen's IM_END)\n",
        "        im_end_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\") # Get ID if needed\n",
        "        if next_token_id.item() == tokenizer.eos_token_id or next_token_id.item() == im_end_token_id:\n",
        "            print(f\"Termination token generated (ID: {next_token_id.item()}) at step {step+1}.\")\n",
        "            break\n",
        "\n",
        "    print(\"Generation finished.\")\n",
        "\n",
        "    # --- Decode the response, excluding the prompt ---\n",
        "    # Extract only the generated token IDs (after the initial prompt)\n",
        "    response_ids = generated_ids[:, prompt_length:]\n",
        "\n",
        "    # Decode the generated part, skipping special tokens used during generation (like padding if any)\n",
        "    # but potentially keeping structure tokens like </think> initially if needed for cleanup\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True) # Usually best for final output\n",
        "\n",
        "    # --- Clean up potential trailing markers ---\n",
        "    # Remove </think> if it appears at the very end\n",
        "    if response_text.strip().endswith(\"</think>\"):\n",
        "       response_text = response_text.rsplit(\"</think>\", 1)[0].strip()\n",
        "\n",
        "    # Further strip any remaining whitespace\n",
        "    return response_text.strip()\n",
        "\n",
        "\n",
        "# @title 6. Run Example Generation\n",
        "prompt = \"Determine all positive integers $n$ for which there exist positive integers $a$, $b$, and $c$ satisfying \\[2a^n + 3b^n = 4c^n.\\]\"\n",
        "print(f\"Original User Prompt: {prompt}\")\n",
        "\n",
        "# --- Run Proxy-Tuned Generation ---\n",
        "if 'model_base' in locals() and model_base is not None:\n",
        "    generated_output = generate_proxy_tuned(\n",
        "        prompt,\n",
        "        max_new_tokens=5000,\n",
        "        alpha=1.0,\n",
        "        temperature=0.6,\n",
        "        top_k=50,\n",
        "        is_math_problem=True,\n",
        "        include_think_prompt=True # Add <think> prompt after assistant marker\n",
        "    )\n",
        "    print(\"\\n--- Proxy-Tuned Output (Using apply_chat_template + R1 Hints) ---\")\n",
        "    print(generated_output)\n",
        "else:\n",
        "    print(\"Models not loaded correctly. Please check the loading logs.\")\n",
        "\n",
        "\n",
        "# @title 7. Qwen2 / DeepSeek R1 Documentation Resources\n",
        "# (Keep documentation links as before)\n",
        "print(\"\\nRelevant documentation links provided in comments and text cell above.\")\n",
        "# Clean up GPU memory\n",
        "# del model_base, model_expert, model_anti_expert"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iZOKEnrbEDY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of one reasoning trace.**\n",
        "\n",
        "--- Proxy-Tuned Output (Using apply_chat_template + R1 Hints) ---\n",
        "\n",
        "Alright, I need to calculate the square root of 12 and 24. Hmm, square roots can sometimes be tricky, especially when dealing with numbers that aren't perfect squares. Let me break this down step by step.\n",
        "\n",
        "Starting with the square root of 12: I know that 12 can be factored into 4 times 3, which are both perfect squares. So, √12 can be written as √(4×3). Using the property of square roots that says √(a×b) = √a × √b, this becomes √4 × √3. I remember that √4 is 2, so multiplying that together gives 2√3. So, √12 equals 2√3. That seems straightforward.\n",
        "\n",
        "Now, moving on to the square root of 24: Similarly, 24 can be factored into 4 times 6. Again, 4 is a perfect square, so √24 can be expressed as √(4×6). Applying the same rule as before, this becomes √4 × √6. √4 is still 2, so this simplifies to 2√6. Got it.\n",
        "\n",
        "Wait a second, maybe I should check if these answers are correct. Let me square 2√3 to see if I get back to 12. (2√3)² = 2² × (√3)² = 4 × 3 = 12. Yep, that works. Similarly, (2√6)² = 4 × 6 = 24. Perfect, both answers are correct.\n",
        "\n",
        "Is there a different way to approach this? Maybe. For √12, I could try dividing it by 2 first, since 2 is a perfect square. √12 = √(4×3) = √4 × √3 = 2√3. Yeah, same result. For √24, dividing by 2 gives √12, which is 2√3. Wait, so √24 = √(4×6) = √4 × √6 = 2√6. Hmm, so actually, √24 can be thought of as 2√6? Interesting.\n",
        "\n",
        "Wait, could √24 also be expressed as √24 ÷ 2? No, that would be √12, not √24. Hmm.\n",
        "\n",
        "I think I've covered multiple ways to calculate these square roots, both factoring them into perfect squares and using the properties of square roots. All paths lead me to the same answers: √12 = 2√3 and √24 = 2√6. That makes sense because 12 is four times 3 and 24 is four times 6, which are both perfect squares multiplied by 4.\n",
        "\n",
        "Maybe I can visualize this. Like, on the number line, 2√3 is approximately 3.464, because √3 is about 1.732, and 2 times 1.732 is 3.464. And √24 is about 4.899, since √6 is approximately 2.449, and 2 times 2.449 is 4.898. Multiplying 3.464 by 3.464 gives 12, as I saw earlier, and 4.898 by 4.898 gives 24. So, the approximate square roots check out.\n",
        "\n",
        "I wonder if these square roots are irrational or rational. Well, 12 isn't a perfect square, so √12 isn't rational. Similarly, 24 isn't a perfect square, so √24 isn't rational. That matches my answers earlier. So, these square roots are irrational numbers.\n",
        "\n",
        "Is there a way to represent these square roots as decimals? Well, √3 is approximately 1.732, so 2√3 is about 3.464. √2 is approximately 1.414, so 2√6 would be? Wait, let me check, √6 is about 2.449, so 2√6 is 4.898. Yeah, these are repeating decimals, but they are irrational. So, they cannot be expressed as finite decimals or fractions.\n",
        "\n",
        "Hmm, okay, so I think that wraps up my thought process on calculating the square roots of 12 and 24. I considered different methods, checked my answers, made sense of the results. Seems solid to me.\n",
        "\n",
        "\n",
        "The square roots of 12 and 24 are calculated by factoring each number into a perfect square times another number, then applying the property of square roots that √(a×b)=√a×√b.\n",
        "\n",
        "For √12:\n",
        "- 12 factors into 4×3, which are both perfect squares.\n",
        "- √12 = √(4×3) = √4 × √3 = 2√3.\n",
        "\n",
        "For √24:\n",
        "- 24 factors into 4×6, which are both perfect squares.\n",
        "- √24 = √(4×6) = √4 × √6 = 2√6.\n",
        "\n",
        "Final answers:\n",
        "√12 = 2√3  \n",
        "√24 = 2√6"
      ],
      "metadata": {
        "id": "ZXxz2G0p8U4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, let me tackle this physics problem step by step. So, we have a ball being thrown upwards with an initial velocity of 40 feet per second. The height of the ball at any time t is given by the equation y = 40t - 16t². We need to find the velocity when t = 2 seconds.\n",
        "\n",
        "First off, I recall that velocity is related to the derivative of the position function. Since the height as a function of time is given by y = 40t - 16t², we can find the velocity by taking the derivative of y with respect to t.\n",
        "\n",
        "So, let me compute the derivative. The derivative of y with respect to t, which is dy/dt, should give us the velocity at any time t.\n",
        "\n",
        "Breaking it down, the derivative of 40t with respect to t is 40, because the slope of a linear function is just its coefficient. Then, the derivative of -16t² with respect to t is -32t. So putting it all together, dy/dt = 40 - 32t.\n",
        "\n",
        "Now, we need to find the velocity at t = 2 seconds. That means we substitute t = 2 into the derivative equation. So, dy/dt at t = 2 is 40 - 32*(2).\n",
        "\n",
        "Calculating that, 32 times 2 is 64, so 40 minus 64 equals... hmm, 40 - 64 is a negative number. Specifically, it's -24. So, the velocity at t = 2 is -24 feet per second.\n",
        "\n",
        "Wait a minute, why is the velocity negative? I think that makes sense because velocity in physics can be a vector quantity, which has both magnitude and direction. If the derivative dy/dt is negative, it means the object is moving in the negative direction, which would be downwards in this case. So, at t = 2 seconds, the ball is descending, and its velocity is decreasing as time passes. Therefore, the negative velocity indicates that the ball is going up for some time, reaches a maximum height, and then starts moving downward.\n",
        "\n",
        "Just to double-check my calculations, let me go back. Starting from the original equation, y = 40t - 16t². Taking the derivative with respect to t, y' =\n"
      ],
      "metadata": {
        "id": "9RqxB43_EEVW"
      }
    }
  ]
}